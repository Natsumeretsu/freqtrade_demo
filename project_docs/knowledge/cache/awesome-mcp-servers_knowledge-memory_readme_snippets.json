[
    {
        "name":  "0xshellming/mcp-summarizer",
        "url":  "https://github.com/0xshellming/mcp-summarizer",
        "kind":  "github",
        "list_desc":  "AI Summarization MCP Server, Support for multiple content types: Plain text, Web pages, PDF documents, EPUB books, HTML content",
        "repo":  "0xshellming/mcp-summarizer",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/0xshellming/mcp-summarizer/master/README.md",
        "snippet":  "# MCP Content Summarizer Server\n\nA Model Context Protocol (MCP) server that provides intelligent summarization capabilities for various types of content using Google\u0027s Gemini 1.5 Pro model. This server can help you generate concise summaries while maintaining key information from different content formats.\n\n\u003ca href=\"https://3min.top\"\u003e\u003cimg width=\"380\" height=\"200\" src=\"/public/imgs/section1_en.jpg\" alt=\"MCP Content Summarizer Server\" /\u003e\u003c/a\u003e\n\n## Powered by 3MinTop\n\nThe summarization service is powered by [3MinTop](https://3min.top), an AI-powered reading tool that helps you understand a chapter\u0027s content in just three minutes. 3MinTop transforms complex content into clear summaries, making learning efficient and helping build lasting reading habits.\n\n## Features\n\n- Universal content summarization using Google\u0027s Gemini 1.5 Pro model\n- Support for multiple content types:\n  - Plain text\n  - Web pages\n  - PDF documents\n  - EPUB books\n  - HTML content\n- Customizable summary length\n- Multi-language support\n- Smart context preservation\n- Dynamic greeting resource for testing\n\n## Getting Started\n\n1. Clone this repository\n2. Install dependencies:\n   ```\n   pnpm install\n   ```\n\n3. Build the project:\n   ```\n   pnpm run build\n   ```\n\n4. Start the server:\n   ```\n   pnpm start\n   ```\n\n## Development\n\n- Use `pnpm run dev` to start the TypeScript compiler in watch mode\n- Modify `src/index.ts` to customize server behavior or add new tools\n\n## Usage with Desktop App\n\nTo integrate this server with a desktop app, add the following to your app\u0027s server configuration:\n\n```js\n{\n  \"mcpServers\": {\n    \"content-summarizer\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"{ABSOLUTE PATH TO FILE HERE}/dist/index.js\"\n      ]\n    }\n  }\n}\n```\n\n## Available Tools\n\n### summarize\n\nSummarizes content from various sources using the following parameters:\n- `content` (string | object): The input content to summarize. Can be:\n  - Text string\n  - URL for web pages\n  - Base64 encoded PDF\n  - EPUB file content\n- `type` (string): Content type (\"text\", \"url\", \"pdf\", \"epub\")\n- `maxLength` (number, optional): Maximum length of the summary in characters (default: 200)\n- `language` (string, optional): Target language for the summary (default: \"en\")\n- `focus` (string, optional): Specific aspect to focus on in the summary\n- `style` (string, optional): Summary style (\"concise\", \"detailed\", \"bullet-points\")\n",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  false,
                        "mentions_api_key":  false,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  true,
                        "storage_markdown":  false,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "apecloud/ApeRAG",
        "url":  "https://github.com/apecloud/ApeRAG",
        "kind":  "github",
        "list_desc":  "Production-ready RAG platform combining Graph RAG, vector search, and full-text search. Best choice for building your own Knowledge Graph and for Context Engineering",
        "repo":  "apecloud/ApeRAG",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/apecloud/ApeRAG/main/README.md",
        "snippet":  "# ApeRAG\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/apecloud/ApeRAG)](https://archestra.ai/mcp-catalog/apecloud__aperag)\n\n**🚀 [Try ApeRAG Live Demo](https://rag.apecloud.com/)** - Experience the full platform capabilities with our hosted demo\n\n\n![HarryPotterKG2.png](docs%2Fimages%2FHarryPotterKG2.png)\n\n![chat2.png](docs%2Fimages%2Fchat2.png)\n\n\nApeRAG is a production-ready RAG (Retrieval-Augmented Generation) platform that combines Graph RAG, vector search, and full-text search with advanced AI agents. Build sophisticated AI applications with hybrid retrieval, multimodal document processing, intelligent agents, and enterprise-grade management features.\n\nApeRAG is the best choice for building your own Knowledge Graph, Context Engineering, and deploying intelligent AI agents that can autonomously search and reason across your knowledge base.\n\n[阅读中文文档](README-zh.md)\n\n- [Quick Start](#quick-start)\n- [Key Features](#key-features)\n- [Kubernetes Deployment (Recommended for Production)](#kubernetes-deployment-recommended-for-production)\n- [Development](./docs/development-guide.md)\n- [Build Docker Image](./docs/build-docker-image.md)\n- [Acknowledgments](#acknowledgments)\n- [License](#license)\n\n## Quick Start\n\n\u003e Before installing ApeRAG, make sure your machine meets the following minimum system requirements:\n\u003e\n\u003e - CPU \u003e= 2 Core\n\u003e - RAM \u003e= 4 GiB\n\u003e - Docker \u0026 Docker Compose\n\nThe easiest way to start ApeRAG is through Docker Compose. Before running the following commands, make sure that [Docker](https://docs.docker.com/get-docker/) and [Docker Compose](https://docs.docker.com/compose/install/) are installed on your machine:\n\n```bash\ngit clone https://github.com/apecloud/ApeRAG.git\ncd ApeRAG\ncp envs/env.template .env\ndocker-compose up -d --pull always\n```\n\nAfter running, you can access ApeRAG in your browser at:\n- **Web Interface**: http://localhost:3000/web/\n- **API Documentation**: http://localhost:8000/docs\n\n#### MCP (Model Context Protocol) Support\n\nApeRAG supports [MCP (Model Context Protocol)](https://modelcontextprotocol.io/) integration, allowing AI assistants to interact with your knowledge base directly. After starting the services, configure your MCP client with:\n\n```json\n{\n  \"mcpServers\": {\n    \"aperag-mcp\": {\n      \"url\": \"https://rag.apecloud.com/mcp/\",\n      \"headers\": {\n        \"Authorization\": \"Bearer your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n**Important**: Replace `http://localhost:8000` with your actual ApeRAG API URL and `your-api-key-here` with a valid API key from your ApeRAG settings.\n\nThe MCP server provides:\n- **Collection browsing**: List and explore your knowledge collections\n- **Hybrid search**: Search using vector, full-text, and graph methods\n- **Intelligent querying**: Ask natural language questions about your documents\n\n#### Enhanced Document Parsing\n\nFor enhanced document parsing capabilities, ApeRAG supports an **advanced document parsing service** powered by MinerU, which provides superior parsing for complex documents, tables, and formulas. \n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cstrong\u003eEnhanced Document Parsing Commands\u003c/strong\u003e\u003c/summary\u003e\n\n```bash\n# Enable advanced document parsing service\nDOCRAY_HOST=http://aperag-docray:8639 docker compose --profile docray up -d",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  true,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  false,
                        "mentions_vector":  true,
                        "mentions_graph":  true,
                        "mentions_web":  false,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  true
                    }
    },
    {
        "name":  "bh-rat/context-awesome",
        "url":  "https://github.com/bh-rat/context-awesome",
        "kind":  "github",
        "list_desc":  "MCP server for querying 8,500+ curated awesome lists (1M+ items) and fetching the best resources for your agent.",
        "repo":  "bh-rat/context-awesome",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/bh-rat/context-awesome/main/README.md",
        "snippet":  "# context-awesome : awesome references for your agents\n\n[![MCP Server](https://img.shields.io/badge/MCP-Server-blue)](https://modelcontextprotocol.io)\n\nA Model Context Protocol (MCP) server that provides access to all the curated awesome lists and their items. It can provide the best resources for your agent from sections of the 8500+ awesome lists on github and more then 1mn+ (growing) awesome row items.\n\n**What are Awesome Lists?** \nAwesome lists are community-curated collections of the best tools, libraries, and resources on any topic - from machine learning frameworks to design tools. By adding this MCP server, your AI agents get instant access to these high-quality, vetted resources instead of relying on random web searches.\n\nPerfect for : \n1. Knowledge worker agents to get the most relevant references for their work\n2. The source for the best learning resources\n3. Deep research can quickly gather a lot of high quality resources for any topic.\n4. Search agents\n\nhttps://github.com/user-attachments/assets/babab991-e4ff-4433-bdb7-eb7032e9cd11\n\n\n## Available Tools\n\n### 1. `find_awesome_section`\n\nDiscovers sections and categories across awesome lists matching your search query.\n\n**Parameters:**\n- `query` (required): Search terms for finding sections\n- `confidence` (optional): Minimum confidence score (0-1, default: 0.3)\n- `limit` (optional): Maximum sections to return (1-50, default: 10)\n\n**Example Usage:**\n\"Give me the best machine learning resources for learning ML related to python in couple of months.\"\n\"What are the best resources for authoring technical books ?\"\n\"Find awesome list sections about React hooks\"\n\"Search for database ORMs in Go awesome lists\"\n\n### 2. `get_awesome_items`\n\nRetrieves items from a specific list or section with token limiting for optimal context usage.\n\n**Parameters:**\n- `listId` or `githubRepo` (one required): Identifier for the list\n- `section` (optional): Category/section name to filter\n- `subcategory` (optional): Subcategory to filter\n- `tokens` (optional): Maximum tokens to return (min: 1000, default: 10000)\n- `offset` (optional): Pagination offset (default: 0)\n\n**Example Usage:**\n```\n\"Show me the testing tools section from awesome-rust\"\n\"Get the next 20 items from awesome-python (offset: 20)\"\n\"Get items from bh-rat/awesome-mcp-enterprise\"\n```\n\n\n## Installation\n\n### Remote Server (Recommended)\n\nContext Awesome is available as a hosted MCP server. No installation required!\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Cursor\u003c/b\u003e\u003c/summary\u003e\n\nGo to: `Settings` -\u003e `Cursor Settings` -\u003e `MCP` -\u003e `Add new global MCP server`\n\n```json\n{\n  \"mcpServers\": {\n    \"context-awesome\": {\n      \"url\": \"https://www.context-awesome.com/api/mcp\"\n    }\n  }\n}\n```\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Claude Code\u003c/b\u003e\u003c/summary\u003e\n\n```sh",
        "signals":  {
                        "mentions_local":  true,
                        "mentions_cloud":  true,
                        "mentions_api_key":  false,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "bitbonsai/mcp-obsidian",
        "url":  "https://github.com/bitbonsai/mcp-obsidian",
        "kind":  "github",
        "list_desc":  "Universal AI bridge for Obsidian vaults using MCP. Provides safe read/write access to notes with 11 comprehensive methods for vault operations including search, batch operations, tag management, and frontmatter handling. Works with Claude, ChatGPT, and any MCP-compatible AI assistant.",
        "repo":  "bitbonsai/mcp-obsidian",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/bitbonsai/mcp-obsidian/main/README.md",
        "snippet":  "\u003cdiv align=\"center\"\u003e\n  \u003cimg width=\"256\" height=\"256\" alt=\"image\" src=\"https://github.com/user-attachments/assets/1e21d898-811b-42c2-a810-bf921dde0f58\" /\u003e\n\u003c/div\u003e\n\n# MCP-Obsidian\n\nA universal AI bridge for Obsidian vaults using the Model Context Protocol (MCP) standard. Connect any MCP-compatible AI assistant to your knowledge base - works with Claude, ChatGPT, and future AI tools. This server provides safe read/write access to your notes while preventing YAML frontmatter corruption.\n\n\u003cdiv align=\"center\"\u003e\n  \n[https://mcp-obsidian.org](https://mcp-obsidian.org)\n\n\u003c/div\u003e\n\n\u003cdiv align=\"center\"\u003e\n\n[![GitHub Stars](https://img.shields.io/github/stars/bitbonsai/mcp-obsidian?style=flat\u0026logo=github\u0026logoColor=white\u0026color=9065ea\u0026labelColor=262626)](https://github.com/bitbonsai/mcp-obsidian)\n[![npm version](https://img.shields.io/npm/v/@mauricio.wolff/mcp-obsidian?style=flat\u0026logo=npm\u0026logoColor=white\u0026color=9065ea\u0026labelColor=262626)](https://www.npmjs.com/package/@mauricio.wolff/mcp-obsidian)\n[![npm downloads](https://img.shields.io/npm/dt/@mauricio.wolff/mcp-obsidian?style=flat\u0026logo=npm\u0026logoColor=white\u0026color=9065ea\u0026labelColor=262626)](https://www.npmjs.com/package/@mauricio.wolff/mcp-obsidian)\n[![GitHub Sponsors](https://img.shields.io/github/sponsors/BitBonsai?style=flat\u0026logo=github\u0026logoColor=white\u0026color=9065ea\u0026labelColor=262626)](https://github.com/sponsors/bitbonsai)\n[![Ko-Fi](https://img.shields.io/badge/Ko--fi-Support%20Me-9065ea?style=flat\u0026logo=ko-fi\u0026logoColor=white\u0026labelColor=262626)](https://ko-fi.com/bitbonsai)\n[![Liberapay](https://img.shields.io/badge/Liberapay-Weekly%20Support-9065ea?style=flat\u0026logo=liberapay\u0026logoColor=white\u0026labelColor=262626)](https://liberapay.com/bitbonsai/)\n\n\u003c/div\u003e\n\n\n\n## Universal Compatibility\nWorks with any MCP-compatible AI assistant including Claude Desktop, Claude Code, ChatGPT Desktop (Enterprise+), IntelliJ IDEA 2025.1+, Cursor IDE, Windsurf IDE, and future AI platforms that adopt the MCP standard.\n\nhttps://github.com/user-attachments/assets/657ac4c6-1cd2-4cc3-829f-fd095a32f71c\n\n## Quick Start (5 minutes)\n\n1. **Install Node.js runtime:**\n   ```bash\n   # Download from https://nodejs.org (v18.0.0 or later)\n   # or use a package manager like nvm, brew, apt, etc.\n   ```\n\n2. **Test the server:**\n\n   If using the published package:\n   ```bash\n   npx @modelcontextprotocol/inspector npx @mauricio.wolff/mcp-obsidian@latest /path/to/your/vault\n   ```\n\n3. **Configure your AI client:**\n\n   **Claude Desktop** - Copy this to `claude_desktop_config.json`:\n   ```json\n   {\n     \"mcpServers\": {\n       \"obsidian\": {\n         \"command\": \"npx\",\n         \"args\": [\"@mauricio.wolff/mcp-obsidian@latest\", \"/path/to/your/vault\"]\n       }\n     }\n   }\n   ```\n\n   **Claude Code** - Copy this to `~/.claude.json`:\n   ```json\n   {\n     \"mcpServers\": {\n       \"obsidian\": {\n         \"command\": \"npx\",\n         \"args\": [\"@mauricio.wolff/mcp-obsidian@latest\", \"/path/to/your/vault\"],\n         \"env\": {}\n       }\n     }\n   }\n   ```\n\n   Replace `/path/to/your/vault` with your actual Obsidian vault path.\n\n   For other platforms, see [detailed configuration guides](#ai-client-configuration) below.\n\n4. **Test with your AI:**\n   - \"List files in my Obsidian vault\"",
        "signals":  {
                        "mentions_local":  true,
                        "mentions_cloud":  false,
                        "mentions_api_key":  false,
                        "mentions_obsidian":  true,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "chatmcp/mcp-server-chatsum",
        "url":  "https://github.com/chatmcp/mcp-server-chatsum",
        "kind":  "github",
        "list_desc":  "Query and summarize your chat messages with AI prompts.",
        "repo":  "chatmcp/mcp-server-chatsum",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/chatmcp/mcp-server-chatsum/main/README.md",
        "snippet":  "# mcp-server-chatsum\n\nThis MCP Server is used to summarize your chat messages.\n\n[中文说明](README_CN.md)\n\n![preview](./preview.png)\n\n\u003e **Before you start**\n\u003e\n\u003e move to [chatbot](./chatbot) directory, follow the [README](./chatbot/README.md) to setup the chat database.\n\u003e\n\u003e start chatbot to save your chat messages.\n\n## Features\n\n### Resources\n\n### Tools\n\n- `query_chat_messages` - Query chat messages\n  - Query chat messages with given parameters\n  - Summarize chat messages based on the query prompt\n\n### Prompts\n\n## Development\n\n1. Set up environment variables:\n\ncreate `.env` file in the root directory, and set your chat database path.\n\n```txt\nCHAT_DB_PATH=path-to/chatbot/data/chat.db\n```\n\n2. Install dependencies:\n\n```bash\npnpm install\n```\n\nBuild the server:\n\n```bash\npnpm build\n```\n\nFor development with auto-rebuild:\n\n```bash\npnpm watch\n```\n\n## Installation\n\nTo use with Claude Desktop, add the server config:\n\nOn MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\nOn Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-chatsum\": {\n      \"command\": \"path-to/bin/node\",\n      \"args\": [\"path-to/mcp-server-chatsum/build/index.js\"],\n      \"env\": {\n        \"CHAT_DB_PATH\": \"path-to/mcp-server-chatsum/chatbot/data/chat.db\"\n      }\n    }\n  }\n}\n```\n\n### Debugging\n\nSince MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:\n\n```bash",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  false,
                        "mentions_api_key":  false,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "contextstream/mcp-server",
        "url":  "https://www.npmjs.com/package/@contextstream/mcp-server",
        "kind":  "github",
        "list_desc":  "Universal persistent memory for AI coding tools. Semantic code search, knowledge graphs, impact analysis, decision tracking, and 60+ MCP tools. Works across Cursor, Claude Code, Windsurf, and any MCP client.",
        "repo":  "sandy-mount/mcp-server",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/sandy-mount/mcp-server/master/README.md",
        "snippet":  "# mcp-server\n\n[![npm version](https://img.shields.io/npm/v/mcp-server.svg)](https://www.npmjs.com/package/mcp-server)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nA simple JSON-RPC 2.0 server implementing the Model-Client-Protocol (MCP). This server provides basic echo functionality and follows the JSON-RPC 2.0 specification.\n\n## Features\n\n- JSON-RPC 2.0 compliant\n- CORS enabled\n- Built-in echo tool\n- Simple initialization protocol\n\n## Quick Start\n\n```bash\nnpx mcp-server\n```\n\nWill run the server on port 4333\n\n## Installation\n\n```bash\nnpm install mcp-server\n```\n",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  false,
                        "mentions_api_key":  false,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  false,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "cameronrye/openzim-mcp",
        "url":  "https://github.com/cameronrye/openzim-mcp",
        "kind":  "github",
        "list_desc":  "Modern, secure MCP server for accessing ZIM format knowledge bases offline. Enables AI models to search and navigate Wikipedia, educational content, and other compressed knowledge archives with smart retrieval, caching, and comprehensive API.",
        "repo":  "cameronrye/openzim-mcp",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/cameronrye/openzim-mcp/main/README.md",
        "snippet":  "# OpenZIM MCP Server\n\n\u003e **Now with Dual Mode Support!** Choose between Simple mode (1 intelligent natural language tool, default) or Advanced mode (15 specialized tools) to match your LLM\u0027s capabilities.\n\n\u003c!-- Build and Quality Badges --\u003e\n[![CI](https://github.com/cameronrye/openzim-mcp/workflows/CI/badge.svg)](https://github.com/cameronrye/openzim-mcp/actions/workflows/test.yml)\n[![codecov](https://codecov.io/gh/cameronrye/openzim-mcp/branch/main/graph/badge.svg)](https://codecov.io/gh/cameronrye/openzim-mcp)\n[![CodeQL](https://github.com/cameronrye/openzim-mcp/workflows/CodeQL%20Security%20Analysis/badge.svg)](https://github.com/cameronrye/openzim-mcp/actions/workflows/codeql.yml)\n[![Security Rating](https://sonarcloud.io/api/project_badges/measure?project=cameronrye_openzim-mcp\u0026metric=security_rating)](https://sonarcloud.io/summary/new_code?id=cameronrye_openzim-mcp)\n\n\u003c!-- Package and Version Badges --\u003e\n[![PyPI version](https://badge.fury.io/py/openzim-mcp.svg)](https://badge.fury.io/py/openzim-mcp)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/openzim-mcp)](https://pypi.org/project/openzim-mcp/)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/openzim-mcp)](https://pypi.org/project/openzim-mcp/)\n[![GitHub release (latest by date)](https://img.shields.io/github/v/release/cameronrye/openzim-mcp)](https://github.com/cameronrye/openzim-mcp/releases)\n\n\u003c!-- Code Quality and Standards --\u003e\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat\u0026labelColor=ef8336)](https://pycqa.github.io/isort/)\n[![Type checked: mypy](https://img.shields.io/badge/type%20checked-mypy-blue)](https://mypy-lang.org/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n\u003c!-- Community and Contribution --\u003e\n[![GitHub issues](https://img.shields.io/github/issues/cameronrye/openzim-mcp)](https://github.com/cameronrye/openzim-mcp/issues)\n[![GitHub pull requests](https://img.shields.io/github/issues-pr/cameronrye/openzim-mcp)](https://github.com/cameronrye/openzim-mcp/pulls)\n[![GitHub contributors](https://img.shields.io/github/contributors/cameronrye/openzim-mcp)](https://github.com/cameronrye/openzim-mcp/graphs/contributors)\n[![GitHub stars](https://img.shields.io/github/stars/cameronrye/openzim-mcp?style=social)](https://github.com/cameronrye/openzim-mcp/stargazers)\n\n## Built for LLM Intelligence\n\n**OpenZIM MCP transforms static ZIM archives into dynamic knowledge engines for Large Language Models.** Unlike basic file readers, this tool provides *intelligent, structured access* that LLMs need to effectively navigate and understand vast knowledge repositories.\n\n **Why LLMs Love OpenZIM MCP:**\n\n- **Smart Navigation**: Browse by namespace (articles, metadata, media) instead of blind searching\n- **Context-Aware Discovery**: Get article structure, relationships, and metadata for deeper understanding\n- **Intelligent Search**: Advanced filtering, auto-complete suggestions, and relevance-ranked results\n- **Performance Optimized**: Cached operations and pagination prevent timeouts on massive archives\n- **Relationship Mapping**: Extract internal/external links to understand content connections\n\nWhether you\u0027re building a research assistant, knowledge chatbot, or content analysis system, OpenZIM MCP gives your LLM the structured access patterns it needs to unlock the full potential of offline knowledge archives. No more fumbling through raw text dumps!\n\n**OpenZIM MCP** is a modern, secure, and high-performance MCP (Model Context Protocol) server that enables AI models to access and search [ZIM format](https://en.wikipedia.org/wiki/ZIM_(file_format)) knowledge bases offline.\n\n[ZIM](https://en.wikipedia.org/wiki/ZIM_(file_format)) (Zeno IMproved) is an open file format developed by the [openZIM project](https://openzim.org/), designed specifically for offline storage and access to website content. The format supports high compression rates using Zstandard compression (default since 2021) and enables fast full-text searching, making it ideal for storing entire Wikipedia content and other large reference materials in relatively compact files. The openZIM project is sponsored by Wikimedia CH and supported by the Wikimedia Foundation, ensuring the format\u0027s continued development and adoption for offline knowledge access, especially in environments without reliable internet connectivity.\n\n## Features\n\n- **Dual Mode Support**: Choose between Simple mode (1 intelligent natural language tool, default) or Advanced mode (15 specialized tools)\n- **Security First**: Comprehensive input validation and path traversal protection\n- **High Performance**: Intelligent caching and optimized ZIM file operations\n- **Smart Retrieval**: Automatic fallback from direct access to search-based retrieval for reliable entry access\n- **Well Tested**: 90%+ test coverage with comprehensive test suite\n- **Modern Architecture**: Modular design with dependency injection\n- **Type Safe**: Full type annotations throughout the codebase\n- **Configurable**: Flexible configuration with validation\n- **Observable**: Structured logging and health monitoring\n\n## Quick Start\n\n### Installation\n\n```bash\n# Install from PyPI (recommended)\npip install openzim-mcp\n```\n\n### Development Installation\n\nFor contributors and developers:\n\n```bash\n# Clone the repository\ngit clone https://github.com/cameronrye/openzim-mcp.git\ncd openzim-mcp\n\n# Install dependencies\nuv sync\n\n# Install development dependencies",
        "signals":  {
                        "mentions_local":  true,
                        "mentions_cloud":  false,
                        "mentions_api_key":  false,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  true,
                        "mentions_web":  true,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "CheMiguel23/MemoryMesh",
        "url":  "https://github.com/CheMiguel23/MemoryMesh",
        "kind":  "github",
        "list_desc":  "Enhanced graph-based memory with a focus on AI role-play and story generation",
        "repo":  "CheMiguel23/MemoryMesh",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/CheMiguel23/MemoryMesh/main/README.md",
        "snippet":  "# MemoryMesh\n[![Release](https://img.shields.io/badge/Release-v0.2.8-blue.svg)](./CHANGELOG.md)\n[![smithery badge](https://smithery.ai/badge/memorymesh)](https://smithery.ai/server/memorymesh)\n![TypeScript](https://img.shields.io/badge/TypeScript-007ACC.svg?logo=typescript\u0026logoColor=white)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n![GitHub Stars](https://img.shields.io/github/stars/CheMiguel23/MemoryMesh.svg?style=social)\n\nMemoryMesh is a knowledge graph server designed for AI models, with a focus on text-based RPGs and interactive storytelling. It helps AI maintain consistent, structured memory across conversations, enabling richer and more dynamic interactions.\n\n*The project is based on the [Knowledge Graph Memory Server](https://github.com/modelcontextprotocol/servers/tree/main/src/memory) from the MCP servers repository and retains its core functionality.*\n\n\u003ca href=\"https://glama.ai/mcp/servers/kf6n6221pd\"\u003e\u003cimg width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/kf6n6221pd/badge\" alt=\"MemoryMesh MCP server\" /\u003e\u003c/a\u003e\n\n## IMPORTANT\nSince `v0.2.7` the default location of schemas was changed to `dist/data/schemas`.\nThis location is not expected to change in the future, but if you are updating from a previous version, make sure to move your schema files to the new location.\n\n## Quick Links\n\n*   [Installation](#installation)\n*   [Example](#example)\n*   [SchemaManager Guide Discussion](https://github.com/CheMiguel23/MemoryMesh/discussions/3)\n*   [MemoryViewer Guide Discussion](https://github.com/CheMiguel23/MemoryMesh/discussions/15)\n\n## Overview\n\nMemoryMesh is a local knowledge graph server that empowers you to build and manage structured information for AI models. While particularly well-suited for text-based RPGs, its adaptable design makes it useful for various applications, including social network simulations, organizational planning, or any scenario involving structured data.\n\n### Key Features\n\n*   **Dynamic Schema-Based Tools:** Define your data structure with schemas, and MemoryMesh automatically generates tools for adding, updating, and deleting data.\n*   **Intuitive Schema Design:** Create schemas that guide the AI in generating and connecting nodes, using required fields, enumerated types, and relationship definitions.\n*   **Metadata for AI Guidance:**  Use metadata to provide context and structure, helping the AI understand the meaning and relationships within your data.\n*   **Relationship Handling:** Define relationships within your schemas to encourage the AI to create connections (edges) between related data points (nodes).\n*   **Informative Feedback:**  Provides error feedback to the AI, enabling it to learn from mistakes and improve its interactions with the knowledge graph.\n*   **Event Support:** An event system tracks operations, providing insights into how the knowledge graph is being modified.\n\n#### Nodes\n\nNodes represent entities or concepts within the knowledge graph. Each node has:\n\n* `name`: A unique identifier.\n* `nodeType`: The type of the node (e.g., `npc`, `artifact`, `location`), defined by your schemas.\n* `metadata`: An array of strings providing descriptive details about the node.\n* `weight`: (Optional) A numerical value between 0 and 1 representing the strength of the relationship, defaulting to 1.\n\n**Example Node:**\n\n```json\n    {\n      \"name\": \"Aragorn\",\n      \"nodeType\": \"player_character\",\n      \"metadata\": [\n        \"Race: Human\",\n        \"Class: Ranger\",\n        \"Skills: Tracking, Swordsmanship\",\n        \"Affiliation: Fellowship of the Ring\"\n      ]\n    }\n```\n\n#### Edges\n\nEdges represent relationships between nodes. Each edge has:\n\n* `from`: The name of the source node.\n* `to`: The name of the target node.\n* `edgeType`: The type of relationship (e.g., `owns`, `located_in`).\n\n```json\n{\n  \"from\": \"Aragorn\",\n  \"to\": \"Andúril\",\n  \"edgeType\": \"owns\"\n}\n```\n\n#### Schemas\n\nSchemas are the heart of MemoryMesh. They define the structure of your data and drive the automatic generation of tools.",
        "signals":  {
                        "mentions_local":  true,
                        "mentions_cloud":  false,
                        "mentions_api_key":  false,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  true,
                        "mentions_web":  true,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "entanglr/zettelkasten-mcp",
        "url":  "https://github.com/entanglr/zettelkasten-mcp",
        "kind":  "github",
        "list_desc":  "A Model Context Protocol (MCP) server that implements the Zettelkasten knowledge management methodology, allowing you to create, link, and search atomic notes through Claude and other MCP-compatible clients.",
        "repo":  "entanglr/zettelkasten-mcp",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/entanglr/zettelkasten-mcp/main/README.md",
        "snippet":  "# Zettelkasten MCP Server\n\nA Model Context Protocol (MCP) server that implements the Zettelkasten knowledge management methodology, allowing you to create, link, explore and synthesize atomic notes through Claude and other MCP-compatible clients.\n\n## What is Zettelkasten?\n\nThe Zettelkasten method is a knowledge management system developed by German sociologist Niklas Luhmann, who used it to produce over 70 books and hundreds of articles. It consists of three core principles:\n\n1. **Atomicity**: Each note contains exactly one idea, making it a discrete unit of knowledge\n2. **Connectivity**: Notes are linked together to create a network of knowledge, with meaningful relationships between ideas\n3. **Emergence**: As the network grows, new patterns and insights emerge that weren\u0027t obvious when the individual notes were created\n\nWhat makes the Zettelkasten approach powerful is how it enables exploration in multiple ways:\n\n- **Vertical exploration**: dive deeper into specific topics by following connections within a subject area.\n- **Horizontal exploration**: discover unexpected relationships between different fields by traversing links that cross domains.\n\nThis structure invites serendipitous discoveries as you follow trails of thought from note to note, all while keeping each piece of information easily accessible through its unique identifier. Luhmann called his system his \"second brain\" or \"communication partner\" - this digital implementation aims to provide similar benefits through modern technology.\n\n## Features\n\n- Create atomic notes with unique timestamp-based IDs\n- Link notes bidirectionally to build a knowledge graph\n- Tag notes for categorical organization\n- Search notes by content, tags, or links\n- Use markdown format for human readability and editing\n- Integrate with Claude through MCP for AI-assisted knowledge management\n- Dual storage architecture (see below)\n- Synchronous operation model for simplified architecture\n\n## Examples\n\n- Knowledge creation: [A small Zettelkasten knowledge network about the Zettelkasten method itself](https://github.com/entanglr/zettelkasten-mcp/discussions/5)\n\n## Note Types\n\nThe Zettelkasten MCP server supports different types of notes:\n\n|Type|Handle|Description|\n|---|---|---|\n|**Fleeting notes**|`fleeting`|Quick, temporary notes for capturing ideas|\n|**Literature notes**|`literature`|Notes from reading material|\n|**Permanent notes**|`permanent`|Well-formulated, evergreen notes|\n|**Structure notes**|`structure`|Index or outline notes that organize other notes|\n|**Hub notes**|`hub`|Entry points to the Zettelkasten on key topics|\n\n## Link Types\n\nThe Zettelkasten MCP server uses a comprehensive semantic linking system that creates meaningful connections between notes. Each link type represents a specific relationship, allowing for a rich, multi-dimensional knowledge graph.\n\n| Primary Link Type | Inverse Link Type | Relationship Description |\n|-------------------|-------------------|--------------------------|\n| `reference` | `reference` | Simple reference to related information (symmetric relationship) |\n| `extends` | `extended_by` | One note builds upon or develops concepts from another |\n| `refines` | `refined_by` | One note clarifies or improves upon another |\n| `contradicts` | `contradicted_by` | One note presents opposing views to another |\n| `questions` | `questioned_by` | One note poses questions about another |\n| `supports` | `supported_by` | One note provides evidence for another |\n| `related` | `related` | Generic relationship (symmetric relationship) |\n\n## Prompting\n\nTo ensure maximum effectiveness, we recommend using a system prompt (\"project instructions\"), project knowledge, and an appropriate chat prompt when asking the LLM to process information, or explore or synthesize your Zettelkasten notes. The `docs` directory in this repository contains the necessary files to get you started:\n\n### System prompts\n\nPick one:\n\n- [system-prompt.md](https://github.com/entanglr/zettelkasten-mcp/blob/main/docs/prompts/system/system-prompt.md)\n- [system-prompt-with-protocol.md](https://github.com/entanglr/zettelkasten-mcp/blob/main/docs/prompts/system/system-prompt-with-protocol.md)\n\n### Project knowledge\n\nFor end users:\n\n- [zettelkasten-methodology-technical.md](https://github.com/entanglr/zettelkasten-mcp/blob/main/docs/project-knowledge/user/zettelkasten-methodology-technical.md)\n- [link-types-in-zettelkasten-mcp-server.md](https://github.com/entanglr/zettelkasten-mcp/blob/main/docs/project-knowledge/user/link-types-in-zettelkasten-mcp-server.md)\n- (more info relevant to your project)\n\n### Chat Prompts",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  false,
                        "mentions_api_key":  false,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  true,
                        "mentions_web":  true,
                        "storage_markdown":  true,
                        "storage_sqlite":  true,
                        "storage_db":  false
                    }
    },
    {
        "name":  "GistPad-MCP",
        "url":  "https://github.com/lostintangent/gistpad-mcp",
        "kind":  "github",
        "list_desc":  "Use GitHub Gists to manage and access your personal knowledge, daily notes, and reusable prompts. This acts as a companion to https://gistpad.dev and the [GistPad VS Code extension](https://aka.ms/gistpad).",
        "repo":  "lostintangent/gistpad-mcp",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/lostintangent/gistpad-mcp/main/README.md",
        "snippet":  "# 📓 GistPad MCP\n\nAn MCP server for managing and sharing your personal knowledge, daily notes, and reuseable prompts via GitHub Gists. It\u0027s a companion to the GistPad [VS Code extension](https://aka.ms/gistpad) and [GistPad.dev](https://gistpad.dev) (for web/mobile), which allows you to access and edit your gists from any MCP-enabled AI product (e.g. GitHub Copilot, Claude Desktop).\n\n- 🏃 [Getting started](#-getting-started)\n- 🛠️ [Included tools](#️-included-tools)\n- 📁 [Included resources](#-included-resources)\n- 💬 [Reusable prompts](#-reusable-prompts)\n- 💻 [CLI reference](#-cli-reference)\n\n## 🏃 Getting started\n\n1. Using VS Code?\n\n   1. Install the [GistPad extension](https://aka.ms/gistpad) and then reload VS Code\n\n      \u003e _Note: This requires VS Code 1.101.0+, so if you\u0027re on an older version, it\u0027s time to upgrade!_\n\n   1. Open the `GistPad` tab and sign-in with your GitHub account. After that, you can begin using GistPad from Copilot chat (in `Agent` mode) without doing any extra setup or token management 💪\n\n1. Other MCP clients...\n\n   1. Generate a personal access token that includes _only_ the `gist` scope: https://github.com/settings/tokens/new\n   1. Add the equivalent of the following to your client\u0027s MCP config file (or via an \"Add MCP server\" GUI/TUI):\n\n      ```json\n      {\n        \"mcpServers\": {\n          \"gistpad\": {\n            \"command\": \"npx\",\n            \"args\": [\"-y\", \"gistpad-mcp\"],\n            \"env\": {\n              \"GITHUB_TOKEN\": \"\u003cYOUR_PAT\u003e\"\n            }\n          }\n        }\n      }\n      ```\n\nOnce your client it setup, you can start having fun with gists + MCP! 🥳 For example, try things like...\n\n1. **Exploring content**\n\n   - `How many gists have I edited this month?`\n   - `What\u0027s the summary of my \u003cfoo\u003e gist?`\n\n1. **Creating content**\n\n   - `Create a new gist about \u003cfoo\u003e`\n   - `Update my \u003cfoo\u003e gist to call out \u003cbar\u003e`\n\n1. **Daily todos**\n\n   - `What are my unfinished todos for today?`\n   - `Add a new todo for \u003cfoo\u003e`\n\n1. **Collaboration**\n\n   - `Add a comment to the \u003cfoo\u003e gist saying \u003cbar\u003e`\n   - `Give me a share URL for the \u003cfoo\u003e gist`\n   - `View my starred gists`\n\n1. **Gist organization**\n\n   - `Archive my gist about \u003cfoo\u003e`\n   - `Add a new \u003cfoo\u003e file to the \u003cbar\u003e gist and migrate the \u003cbaz\u003e content into it`\n\n1. **Reusable prompts**\n\n   - `Create a new prompt that searches the web ofor a specified manga series and then provides a summary about it`\n   - `Delete my prompt about \u003cfoo\u003e`\n\n## 🛠️ Included tools\n\n### Gist management\n\n- `list_gists` - List all of your gists (excluding daily notes and archived gists).\n- `get_gist` - Get the contents of a gist by ID.\n- `create_gist` - Create a new gist with a specified description and initial file contents.\n- `delete_gist` - Delete a gist by ID.",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  false,
                        "mentions_api_key":  false,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "graphlit-mcp-server",
        "url":  "https://github.com/graphlit/graphlit-mcp-server",
        "kind":  "github",
        "list_desc":  "Ingest anything from Slack, Discord, websites, Google Drive, Linear or GitHub into a Graphlit project - and then search and retrieve relevant knowledge within an MCP client like Cursor, Windsurf or Cline.",
        "repo":  "graphlit/graphlit-mcp-server",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/graphlit/graphlit-mcp-server/main/README.md",
        "snippet":  "[![npm version](https://badge.fury.io/js/graphlit-mcp-server.svg)](https://badge.fury.io/js/graphlit-mcp-server)\n[![smithery badge](https://smithery.ai/badge/@graphlit/graphlit-mcp-server)](https://smithery.ai/server/@graphlit/graphlit-mcp-server)\n\n# Model Context Protocol (MCP) Server for Graphlit Platform\n\n## Overview\n\nThe Model Context Protocol (MCP) Server enables integration between MCP clients and the Graphlit service. This document outlines the setup process and provides a basic example of using the client.\n\nIngest anything from Slack, Discord, websites, Google Drive, email, Jira, Linear or GitHub into a Graphlit project - and then search and retrieve relevant knowledge within an MCP client like Cursor, Windsurf, Goose or Cline.\n\nYour Graphlit project acts as a searchable, and RAG-ready knowledge base across all your developer and product management tools.\n\nDocuments (PDF, DOCX, PPTX, etc.) and HTML web pages will be extracted to Markdown upon ingestion. Audio and video files will be transcribed upon ingestion.\n\nWeb crawling and web search are built-in as MCP tools, with no need to integrate other tools like Firecrawl, Exa, etc. separately.\n\nYou can read more about the MCP Server use cases and features on our [blog](https://www.graphlit.com/blog/graphlit-mcp-server).\n\nWatch our latest [YouTube video](https://www.youtube.com/watch?v=Or-QqonvcAs\u0026t=4s) on using the Graphlit MCP Server with the Goose MCP client.\n\nFor any questions on using the MCP Server, please join our [Discord](https://discord.gg/ygFmfjy3Qx) community and post on the #mcp channel.\n\n\u003ca href=\"https://glama.ai/mcp/servers/fscrivteod\"\u003e\n  \u003cimg width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/fscrivteod/badge\" alt=\"graphlit-mcp-server MCP server\" /\u003e\n\u003c/a\u003e\n\n## Tools\n\n### Retrieval\n\n- Query Contents\n- Query Collections\n- Query Feeds\n- Query Conversations\n- Retrieve Relevant Sources\n- Retrieve Similar Images\n- Visually Describe Image\n\n### RAG\n\n- Prompt LLM Conversation\n\n### Extraction\n\n- Extract Structured JSON from Text\n\n### Publishing\n\n- Publish as Audio (ElevenLabs Audio)\n- Publish as Image (OpenAI Image Generation)\n\n### Ingestion\n\n- Files\n- Web Pages\n- Messages\n- Posts\n- Emails\n- Issues\n- Text\n- Memory (Short-Term)\n\n### Data Connectors\n\n- Microsoft Outlook email\n- Google Mail\n- Notion\n- Reddit\n- Linear\n- Jira\n- GitHub Issues\n- Google Drive\n- OneDrive\n- SharePoint\n- Dropbox\n- Box\n- GitHub\n- Slack\n- Microsoft Teams",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  false,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  true,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "hannesrudolph/mcp-ragdocs",
        "url":  "https://github.com/hannesrudolph/mcp-ragdocs",
        "kind":  "github",
        "list_desc":  "An MCP server implementation that provides tools for retrieving and processing documentation through vector search, enabling AI assistants to augment their responses with relevant documentation context",
        "repo":  "hannesrudolph/mcp-ragdocs",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/hannesrudolph/mcp-ragdocs/main/README.md",
        "snippet":  "# RAG Documentation MCP Server\n\nAn MCP server implementation that provides tools for retrieving and processing documentation through vector search, enabling AI assistants to augment their responses with relevant documentation context.\n\n\u003ca href=\"https://glama.ai/mcp/servers/54hsrjhmq9\"\u003e\u003cimg width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/54hsrjhmq9/badge\" alt=\"mcp-ragdocs MCP server\" /\u003e\u003c/a\u003e\n\n## Features\n\n- Vector-based documentation search and retrieval\n- Support for multiple documentation sources\n- Semantic search capabilities\n- Automated documentation processing\n- Real-time context augmentation for LLMs\n\n## Tools\n\n### search_documentation\nSearch through stored documentation using natural language queries. Returns matching excerpts with context, ranked by relevance.\n\n**Inputs:**\n- `query` (string): The text to search for in the documentation. Can be a natural language query, specific terms, or code snippets.\n- `limit` (number, optional): Maximum number of results to return (1-20, default: 5). Higher limits provide more comprehensive results but may take longer to process.\n\n### list_sources\nList all documentation sources currently stored in the system. Returns a comprehensive list of all indexed documentation including source URLs, titles, and last update times. Use this to understand what documentation is available for searching or to verify if specific sources have been indexed.\n\n### extract_urls\nExtract and analyze all URLs from a given web page. This tool crawls the specified webpage, identifies all hyperlinks, and optionally adds them to the processing queue.\n\n**Inputs:**\n- `url` (string): The complete URL of the webpage to analyze (must include protocol, e.g., https://). The page must be publicly accessible.\n- `add_to_queue` (boolean, optional): If true, automatically add extracted URLs to the processing queue for later indexing. Use with caution on large sites to avoid excessive queuing.\n\n### remove_documentation\nRemove specific documentation sources from the system by their URLs. The removal is permanent and will affect future search results.\n\n**Inputs:**\n- `urls` (string[]): Array of URLs to remove from the database. Each URL must exactly match the URL used when the documentation was added.\n\n### list_queue\nList all URLs currently waiting in the documentation processing queue. Shows pending documentation sources that will be processed when run_queue is called. Use this to monitor queue status, verify URLs were added correctly, or check processing backlog.\n\n### run_queue\nProcess and index all URLs currently in the documentation queue. Each URL is processed sequentially, with proper error handling and retry logic. Progress updates are provided as processing occurs. Long-running operations will process until the queue is empty or an unrecoverable error occurs.\n\n### clear_queue\nRemove all pending URLs from the documentation processing queue. Use this to reset the queue when you want to start fresh, remove unwanted URLs, or cancel pending processing. This operation is immediate and permanent - URLs will need to be re-added if you want to process them later.\n\n## Usage\n\nThe RAG Documentation tool is designed for:\n\n- Enhancing AI responses with relevant documentation\n- Building documentation-aware AI assistants\n- Creating context-aware tooling for developers\n- Implementing semantic documentation search\n- Augmenting existing knowledge bases\n\n## Configuration\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"rag-docs\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@hannesrudolph/mcp-ragdocs\"\n      ],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"\",\n        \"QDRANT_URL\": \"\",\n        \"QDRANT_API_KEY\": \"\"\n      }\n    }\n  }",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  false,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  false,
                        "mentions_vector":  true,
                        "mentions_graph":  false,
                        "mentions_web":  true,
                        "storage_markdown":  false,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "jinzcdev/markmap-mcp-server",
        "url":  "https://github.com/jinzcdev/markmap-mcp-server",
        "kind":  "github",
        "list_desc":  "An MCP server built on [markmap](https://github.com/markmap/markmap) that converts **Markdown** to interactive **mind maps**. Supports multi-format exports (PNG/JPG/SVG), live browser preview, one-click Markdown copy, and dynamic visualization features.",
        "repo":  "jinzcdev/markmap-mcp-server",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/jinzcdev/markmap-mcp-server/main/README.md",
        "snippet":  "# Markmap MCP Server\n\n![Sample Mindmap](./docs/markmap.svg)\n\n[![NPM Version](https://img.shields.io/npm/v/@jinzcdev/markmap-mcp-server.svg)](https://www.npmjs.com/package/@jinzcdev/markmap-mcp-server)\n[![GitHub License](https://img.shields.io/github/license/jinzcdev/markmap-mcp-server.svg)](LICENSE)\n[![Smithery Badge](https://smithery.ai/badge/@jinzcdev/markmap-mcp-server)](https://smithery.ai/server/@jinzcdev/markmap-mcp-server)\n[![中文文档](https://img.shields.io/badge/中文文档-点击查看-blue)](README_zh-CN.md)\n[![Stars](https://img.shields.io/github/stars/jinzcdev/markmap-mcp-server)](https://github.com/jinzcdev/markmap-mcp-server)\n\nMarkmap MCP Server is based on the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) that allows one-click conversion of Markdown text to interactive mind maps, built on the open source project [markmap](https://github.com/markmap/markmap). The generated mind maps support rich interactive operations and can be exported in various image formats.\n\n\u003e 🎉 **Explore More Mind Mapping Tools**\n\u003e\n\u003e Try [MarkXMind](https://github.com/jinzcdev/markxmind) - An online editor that creates complex mind maps using simple XMindMark syntax. It supports real-time preview, multi-format export (.xmind/.svg/.png), importing existing XMind files. [Try it now](https://markxmind.js.org/)!\n\n## Features\n\n- 🌠 **Markdown to Mind Map**: Convert Markdown text to interactive mind maps\n- 🖼️ **Multi-format Export**: Support for exporting as PNG, JPG, and SVG images\n- 🔄 **Interactive Operations**: Support for zooming, expanding/collapsing nodes, and other interactive features\n- 📋 **Markdown Copy**: One-click copy of the original Markdown content\n- 🌐 **Automatic Browser Preview**: Option to automatically open generated mind maps in the browser\n\n## Prerequisites\n\n1. Node.js (v20 or above)\n\n## Installation\n\n### Installing via Smithery\n\nTo install Markmap MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@jinzcdev/markmap-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @jinzcdev/markmap-mcp-server --client claude\n```\n\n### Manual Installation\n\n```bash\n# Install from npm\nnpm install @jinzcdev/markmap-mcp-server -g\n\n# Basic run\nnpx -y @jinzcdev/markmap-mcp-server\n\n# Specify output directory\nnpx -y @jinzcdev/markmap-mcp-server --output /path/to/output/directory\n```\n\nAlternatively, you can clone the repository and run locally:\n\n```bash\n# Clone the repository\ngit clone https://github.com/jinzcdev/markmap-mcp-server.git\n\n# Navigate to the project directory\ncd markmap-mcp-server\n\n# Build project\nnpm install \u0026\u0026 npm run build\n\n# Run the server\nnode build/index.js\n```\n\n## Usage\n\nAdd the following configuration to your MCP client configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"markmap\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@jinzcdev/markmap-mcp-server\"],\n      \"env\": {\n        \"MARKMAP_DIR\": \"/path/to/output/directory\"",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  false,
                        "mentions_api_key":  false,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "kaliaboi/mcp-zotero",
        "url":  "https://github.com/kaliaboi/mcp-zotero",
        "kind":  "github",
        "list_desc":  "A connector for LLMs to work with collections and sources on your Zotero Cloud",
        "repo":  "kaliaboi/mcp-zotero",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/kaliaboi/mcp-zotero/main/README.md",
        "snippet":  "# MCP Zotero\n\n![NPM Version](https://img.shields.io/npm/v/mcp-zotero) [![smithery badge](https://smithery.ai/badge/mcp-zotero)](https://smithery.ai/server/mcp-zotero)\n\nA Model Context Protocol server for Zotero integration that allows Claude to interact with your Zotero library.\n\n\u003ca href=\"https://glama.ai/mcp/servers/mjvu0xzzzz\"\u003e\u003cimg width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/mjvu0xzzzz/badge\" alt=\"Zotero MCP server\" /\u003e\u003c/a\u003e\n\n## Setup\n\n1. Get your Zotero credentials:\n\n   ```bash\n   # First, create an API key at https://www.zotero.org/settings/keys\n   # Then use it to get your user ID:\n   curl -H \"Zotero-API-Key: YOUR_API_KEY\" https://api.zotero.org/keys/current\n   ```\n\n   The response will look like:\n\n   ```json\n   {\n     \"userID\": 123456,\n     \"username\": \"your_username\",\n     \"access\": {\n       \"user\": {\n         \"library\": true,\n         \"files\": true,\n         \"notes\": true,\n         \"write\": true\n       }\n     }\n   }\n   ```\n\n   The `userID` value is what you need.\n\n2. Set environment variables:\n\n   ```bash\n   export ZOTERO_API_KEY=\"your-api-key\"\n   export ZOTERO_USER_ID=\"user-id-from-curl\"\n   ```\n\n3. Verify your credentials:\n\n   ```bash\n   # Test that your credentials work:\n   curl -H \"Zotero-API-Key: $ZOTERO_API_KEY\" \\\n        \"https://api.zotero.org/users/$ZOTERO_USER_ID/collections\"\n   ```\n\n   You should see your collections list in the response.\n\n4. Install and run:\n\n   ```bash\n   # Install globally (recommended)\n   npm install -g mcp-zotero\n   mcp-zotero\n\n   # Or run directly with npx\n   npx mcp-zotero\n   ```\n\n## Integration with Claude Desktop\n\nTo use this server with Claude Desktop, add the following to your Claude Desktop configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"zotero\": {\n      \"command\": \"mcp-zotero\",\n      \"env\": {\n        \"ZOTERO_API_KEY\": YOUR_API_KEY,\n        \"ZOTERO_USER_ID\": YOUR_USER_ID\n      }\n    }\n  }",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  false,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  false,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "pallaprolus/mendeley-mcp",
        "url":  "https://github.com/pallaprolus/mendeley-mcp",
        "kind":  "github",
        "list_desc":  "MCP server for Mendeley reference manager. Search your library, browse folders, get document metadata, search the global catalog, and add papers to your collection.",
        "repo":  "pallaprolus/mendeley-mcp",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/pallaprolus/mendeley-mcp/main/README.md",
        "snippet":  "\u003cp align=\"center\"\u003e\n  \u003cimg src=\"https://raw.githubusercontent.com/pallaprolus/mendeley-mcp/main/mendeley-mcp.png\" alt=\"Mendeley MCP Logo\" width=\"200\"\u003e\n\u003c/p\u003e\n\n# Mendeley MCP Server\n\nAn [MCP (Model Context Protocol)](https://modelcontextprotocol.io/) server that connects your Mendeley reference library to LLM applications like Claude Desktop, Cursor, and other MCP-compatible clients.\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![PyPI version](https://img.shields.io/pypi/v/mendeley-mcp.svg)](https://pypi.org/project/mendeley-mcp/)\n[![PyPI Downloads](https://static.pepy.tech/personalized-badge/mendeley-mcp?period=total\u0026units=international_system\u0026left_color=black\u0026right_color=green\u0026left_text=downloads)](https://pepy.tech/project/mendeley-mcp)\n[![Docker](https://img.shields.io/badge/docker-available-blue.svg)](https://github.com/pallaprolus/mendeley-mcp/pkgs/container/mendeley-mcp)\n\n\u003ca href=\"https://glama.ai/mcp/servers/@pallaprolus/mendeley-mcp\"\u003e\n  \u003cimg width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@pallaprolus/mendeley-mcp/badge\" alt=\"Mendeley MCP server on Glama\" /\u003e\n\u003c/a\u003e\n\n## Features\n\n- **Search your library** - Find papers by title, author, abstract, or notes\n- **Browse folders** - Navigate your collection structure\n- **Get full metadata** - Retrieve complete document details including abstracts\n- **Search global catalog** - Access Mendeley\u0027s 100M+ paper database\n- **DOI lookup** - Find papers by their DOI\n- **Add documents** - Create new entries in your library\n\n## Prerequisites\n\n1. **Mendeley Account** - Sign up at [mendeley.com](https://www.mendeley.com/) (uses Elsevier authentication)\n2. **Mendeley API App** - Register at [dev.mendeley.com/myapps.html](https://dev.mendeley.com/myapps.html)\n   - Sign in with your Elsevier credentials\n   - Click \"Register a new app\"\n   - Set redirect URL to `http://localhost:8585/callback`\n   - Select \"Authorization code\" flow (not Legacy)\n   - Note your **Client ID** and **Client Secret**\n\n## Installation\n\n### Using pip\n\n```bash\npip install mendeley-mcp\n```\n\n### Using uv (recommended)\n\n```bash\nuv tool install mendeley-mcp\n```\n\n### Using Docker\n\n```bash\ndocker run -it \\\n  -e MENDELEY_CLIENT_ID=\"your-client-id\" \\\n  -e MENDELEY_CLIENT_SECRET=\"your-client-secret\" \\\n  -e MENDELEY_REFRESH_TOKEN=\"your-refresh-token\" \\\n  ghcr.io/pallaprolus/mendeley-mcp\n```\n\nOr build locally:\n```bash\ngit clone https://github.com/pallaprolus/mendeley-mcp.git\ncd mendeley-mcp\ndocker build -t mendeley-mcp .\n```\n\n### From source\n\n```bash\ngit clone https://github.com/pallaprolus/mendeley-mcp.git\ncd mendeley-mcp\npip install -e .\n```\n\n## Quick Start\n\n### 1. Authenticate with Mendeley\n",
        "signals":  {
                        "mentions_local":  true,
                        "mentions_cloud":  false,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  true,
                        "storage_markdown":  false,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "louis030195/easy-obsidian-mcp",
        "url":  "https://github.com/louis030195/easy-obsidian-mcp",
        "kind":  "github",
        "list_desc":  "Interact with Obsidian vaults for knowledge management. Create, read, update, and search notes. Works with local Obsidian vaults using filesystem access.",
        "repo":  "louis030195/easy-obsidian-mcp",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/louis030195/easy-obsidian-mcp/main/README.md",
        "snippet":  "# Obsidian MCP Server\n\n\u003cimg width=\"1495\" height=\"429\" alt=\"image\" src=\"https://github.com/user-attachments/assets/ed3f9f26-ddbd-4baf-8fce-34b44cc3c30f\" /\u003e\n\n\nConnect Claude, ChatGPT, and other AI assistants to your Obsidian vault.\n\n---\n\n\u003cdiv align=\"center\"\u003e\n\n### 💖 Support This Project\n\n**If you find this MCP server useful, please consider supporting its development!**\n\n[![Support via Stripe](https://img.shields.io/badge/Support-Stripe-635bff?style=for-the-badge\u0026logo=stripe\u0026logoColor=white)](https://buy.stripe.com/fZu8wP2n7a34fix2LKgA800)\n\n[**👉 Click here to support this project**](https://buy.stripe.com/fZu8wP2n7a34fix2LKgA800)\n\n*Your support helps maintain and improve this tool. Thank you!* 🙏\n\n\u003c/div\u003e\n\n---\n\n## Quick Start\n\n### 1. Install in your AI app\n\n**Claude Desktop:**\nAdd to `~/Library/Application Support/Claude/claude_desktop_config.json` (Mac) or `%APPDATA%\\Claude\\claude_desktop_config.json` (Windows):\n```json\n{\n  \"mcpServers\": {\n    \"obsidian\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@louis030195/mcp-obsidian\"],\n      \"env\": {\n        \"OBSIDIAN_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n**Claude Code:**\n```bash\nclaude mcp add obsidian \"npx -y @louis030195/mcp-obsidian\" -s user -e OBSIDIAN_API_KEY=\"your-api-key-here\"\n```\n\n### 2. Enable Obsidian API (Optional - for full features)\n- Open Obsidian → Settings → Community plugins\n- Turn off Restricted mode → Search \"Local REST API\" → Install \u0026 Enable\n- Copy the API key and add to your config:\n  ```json\n  \"env\": {\n    \"OBSIDIAN_API_KEY\": \"your-api-key-here\"\n  }\n  ```\n\nThat\u0027s it! Your AI can now search and read your Obsidian notes.\n\n## Other AI Apps\n\n**Cursor:** Add to settings (Cmd+Shift+P → \"Preferences: Open User Settings (JSON)\"):\n```json\n{\n  \"mcpServers\": {\n    \"obsidian\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@louis030195/mcp-obsidian\"],\n      \"env\": {\n        \"OBSIDIAN_API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n## What It Can Do",
        "signals":  {
                        "mentions_local":  true,
                        "mentions_cloud":  false,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  true,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  false,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "mem0ai/mem0-mcp",
        "url":  "https://github.com/mem0ai/mem0-mcp",
        "kind":  "github",
        "list_desc":  "A Model Context Protocol server for Mem0 that helps manage coding preferences and patterns, providing tools for storing, retrieving and semantically handling code implementations, best practices and technical documentation in IDEs like Cursor and Windsurf",
        "repo":  "mem0ai/mem0-mcp",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/mem0ai/mem0-mcp/main/README.md",
        "snippet":  "# Mem0 MCP Server\n\n[![PyPI version](https://img.shields.io/pypi/v/mem0-mcp-server.svg)](https://pypi.org/project/mem0-mcp-server/) [![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE) [![smithery badge](https://smithery.ai/badge/@mem0ai/mem0-memory-mcp)](https://smithery.ai/server/@mem0ai/mem0-memory-mcp)\n\n`mem0-mcp-server` wraps the official [Mem0](https://mem0.ai) Memory API as a Model Context Protocol (MCP) server so any MCP-compatible client (Claude Desktop, Cursor, custom agents) can add, search, update, and delete long-term memories.\n\n## Tools\n\nThe server exposes the following tools to your LLM:\n\n| Tool                  | Description                                                                       |\n| --------------------- | --------------------------------------------------------------------------------- |\n| `add_memory`          | Save text or conversation history (or explicit message objects) for a user/agent. |\n| `search_memories`     | Semantic search across existing memories (filters + limit supported).             |\n| `get_memories`        | List memories with structured filters and pagination.                             |\n| `get_memory`          | Retrieve one memory by its `memory_id`.                                           |\n| `update_memory`       | Overwrite a memory\u0027s text once the user confirms the `memory_id`.                 |\n| `delete_memory`       | Delete a single memory by `memory_id`.                                            |\n| `delete_all_memories` | Bulk delete all memories in the confirmed scope (user/agent/app/run).             |\n| `delete_entities`     | Delete a user/agent/app/run entity (and its memories).                            |\n| `list_entities`       | Enumerate users/agents/apps/runs stored in Mem0.                                  |\n\nAll responses are JSON strings returned directly from the Mem0 API.\n\n## Usage Options\n\nThere are three ways to use the Mem0 MCP Server:\n\n1. **Python Package** - Install and run locally using `uvx` with any MCP client\n2. **Docker** - Containerized deployment that creates an `/mcp` HTTP endpoint\n3. **Smithery** - Remote hosted service for managed deployments\n\n## Quick Start\n\n### Installation\n\n```bash\nuv pip install mem0-mcp-server\n```\n\nOr with pip:\n\n```bash\npip install mem0-mcp-server\n```\n\n### Client Configuration\n\nAdd this configuration to your MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"mem0\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mem0-mcp-server\"],\n      \"env\": {\n        \"MEM0_API_KEY\": \"m0-...\",\n        \"MEM0_DEFAULT_USER_ID\": \"your-handle\"\n      }\n    }\n  }\n}\n```\n\n### Test with the Python Agent\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cstrong\u003eClick to expand: Test with the Python Agent\u003c/strong\u003e\u003c/summary\u003e\n\nTo test the server immediately, use the included Pydantic AI agent:\n\n```bash\n# Install the package\npip install mem0-mcp-server\n# Or with uv\nuv pip install mem0-mcp-server\n\n# Set your API keys\nexport MEM0_API_KEY=\"m0-...\"",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  true,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  true,
                        "mentions_web":  false,
                        "storage_markdown":  false,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "modelcontextprotocol/server-memory",
        "url":  "https://github.com/modelcontextprotocol/servers-archived/tree/main/src/memory",
        "kind":  "github",
        "list_desc":  "Knowledge graph-based persistent memory system for maintaining context",
        "repo":  "modelcontextprotocol/servers-archived",
        "branch":  "main",
        "subdir":  "src/memory",
        "readme_found":  false,
        "readme_url":  null,
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  false,
                        "mentions_api_key":  false,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  false,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "MWGMorningwood/Central-Memory-MCP",
        "url":  "https://github.com/MWGMorningwood/Central-Memory-MCP",
        "kind":  "github",
        "list_desc":  "An Azure PaaS-hostable MCP server that provides a workspace-grounded knowledge graph for multiple developers using Azure Functions MCP triggers and Table storage.",
        "repo":  "MWGMorningwood/Central-Memory-MCP",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/MWGMorningwood/Central-Memory-MCP/main/README.md",
        "snippet":  "# Central Memory MCP Server (.NET 10 Azure Functions)\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/MWGMorningwood/Central-Memory-MCP)](https://archestra.ai/mcp-catalog/mwgmorningwood__central-memory-mcp)\n\nModel Context Protocol (MCP) compliant memory \u0026 knowledge graph server implemented in .NET 10 (Azure Functions isolated worker). Provides durable project memory (entities, relations, observations) for AI assistants with workspace isolation and simple HTTP tool endpoints.\n\n## Implemented MCP Tools\nCurrent function set (alpha stage):\n- read_graph - returns all entities for a workspace (relations currently queried separately and joined)\n- upsert_entity - create or update entity (preserves existing Id if name exists)\n- upsert_relation - create or update relation between two entities (requires GUIDs or resolvable names)\n- get_entity_relations - list relations originating from a specific entity\n- Health \u0026 Ready endpoints (/api/health, /api/ready)\n\nPlanned (not yet implemented): search_entities, search_relations, stats, temporal, batch operations, merge/detect duplicates.\n\n## Data Model\n- Entities table: PartitionKey = WorkspaceName, RowKey = Guid (Id)\n- Relations table: PartitionKey = WorkspaceName, RowKey = Guid (Id)\n- Workspaces table (future expansion)\n\nObservations stored as a single delimited string (\"||\") internally; split into List\u003cstring\u003e at read time.\n\n## Quick Start\n```bash\ndotnet restore\ndotnet build\nfunc start --port 7071\ncurl http://localhost:7071/api/health\n```\nTo read the graph, invoke the read_graph MCP tool from the client and supply the workspaceName parameter.\n\n## Directory Layout\n```mermaid\nflowchart TD\n    A[CentralMemoryMcp.Functions]\n    A --\u003e P[Program.cs]\n    A --\u003e SR[ServiceRegistration.cs]\n    A --\u003e F[Functions]\n    F --\u003e GF[GraphFunctions.cs]\n    F --\u003e HF[HealthFunctions.cs]\n    A --\u003e S[Services]\n    S --\u003e KGS[KnowledgeGraphService.cs]\n    A --\u003e ST[Storage]\n    ST --\u003e TSS[TableStorageService.cs]\n    A --\u003e M[Models]\n    M --\u003e GM[GraphModels.cs]\n    A --\u003e CFG[appsettings.json]\n    A --\u003e HOST[host.json]\n```\n\n## Usage Notes\n- Use workspaceName consistently; workspaceId in docs replaced.\n- Upsert preserves entity identity by lookup on (WorkspaceName + Name).\n- Relation upsert requires entity GUIDs or resolves names; fails if names missing.\n\n## Logging \u0026 Telemetry\nAdd Application Insights connection to capture request latency \u0026 storage dependency tracking (future enhancement).\n\n## Roadmap\nAdd search, pagination, stats aggregation, batch operations, duplicate detection, merge strategy, semantic vector layer.\n\n## License\n[MIT](./LICENSE).\n",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  false,
                        "mentions_api_key":  false,
                        "mentions_obsidian":  false,
                        "mentions_vector":  true,
                        "mentions_graph":  true,
                        "mentions_web":  false,
                        "storage_markdown":  false,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "nonatofabio/local-faiss-mcp",
        "url":  "https://github.com/nonatofabio/local_faiss_mcp",
        "kind":  "github",
        "list_desc":  "Local FAISS vector database for RAG with document ingestion (PDF/TXT/MD/DOCX), semantic search, re-ranking, and CLI tools for indexing and querying",
        "repo":  "nonatofabio/local_faiss_mcp",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/nonatofabio/local_faiss_mcp/main/README.md",
        "snippet":  "# Local FAISS MCP Server\n\n\u003c!-- mcp-name: io.github.nonatofabio/local-faiss-mcp --\u003e\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n[![Tests](https://github.com/nonatofabio/local_faiss_mcp/workflows/Tests/badge.svg)](https://github.com/nonatofabio/local_faiss_mcp/actions)\n[![PyPI version](https://badge.fury.io/py/local-faiss-mcp.svg)](https://badge.fury.io/py/local-faiss-mcp)\n\nA Model Context Protocol (MCP) server that provides local vector database functionality using FAISS for Retrieval-Augmented Generation (RAG) applications.\n\n![demo](./static/demo.gif)\n\n## Features\n\n### Core Capabilities\n- **Local Vector Storage**: Uses FAISS for efficient similarity search without external dependencies\n- **Document Ingestion**: Automatically chunks and embeds documents for storage\n- **Semantic Search**: Query documents using natural language with sentence embeddings\n- **Persistent Storage**: Indexes and metadata are saved to disk\n- **MCP Compatible**: Works with any MCP-compatible AI agent or client\n\n### v0.2.0 Highlights\n- **CLI Tool**: `local-faiss` command for standalone indexing and search\n- **Document Formats**: Native PDF/TXT/MD support, DOCX/HTML/EPUB with pandoc\n- **Re-ranking**: Two-stage retrieve and rerank for better results\n- **Custom Embeddings**: Choose any Hugging Face embedding model\n- **MCP Prompts**: Built-in prompts for answer extraction and summarization\n\n## Quickstart\n\n```bash\n# Install\npip install local-faiss-mcp\n\n# Index documents\nlocal-faiss index document.pdf\n\n# Search\nlocal-faiss search \"What is this document about?\"\n```\n\nOr use with Claude Code - configure MCP client (see [Configuration](#configuration-with-mcp-clients)) and try:\n\n```\nUse the ingest_document tool with: ./path/to/document.pdf\nThen use query_rag_store to search for: \"How does FAISS perform similarity search?\"\n```\n\nClaude will retrieve relevant document chunks from your vector store and use them to answer your question.\n\n## Installation\n\n⚡️ **Upgrading?** Run `pip install --upgrade local-faiss-mcp`\n\n### From PyPI (Recommended)\n\n```bash\npip install local-faiss-mcp\n```\n\n### Optional: Extended Format Support\n\nFor DOCX, HTML, EPUB, and 40+ additional formats, install pandoc:\n\n```bash\n# macOS\nbrew install pandoc\n\n# Linux\nsudo apt install pandoc\n\n# Or download from: https://pandoc.org/installing.html\n```\n\n**Note**: PDF, TXT, and MD work without pandoc.\n\n### From Source\n\n```bash",
        "signals":  {
                        "mentions_local":  true,
                        "mentions_cloud":  false,
                        "mentions_api_key":  false,
                        "mentions_obsidian":  false,
                        "mentions_vector":  true,
                        "mentions_graph":  false,
                        "mentions_web":  true,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "pi22by7/In-Memoria",
        "url":  "https://github.com/pi22by7/In-Memoria",
        "kind":  "github",
        "list_desc":  "Persistent intelligence infrastructure for agentic development that gives AI coding assistants cumulative memory and pattern learning. Hybrid TypeScript/Rust implementation with local-first storage using SQLite + SurrealDB for semantic analysis and incremental codebase understanding.",
        "repo":  "pi22by7/In-Memoria",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/pi22by7/In-Memoria/main/README.md",
        "snippet":  "# In Memoria\n\n[![npm version](https://img.shields.io/npm/v/in-memoria.svg)](https://www.npmjs.com/package/in-memoria)\n[![npm downloads](https://img.shields.io/npm/dm/in-memoria.svg)](https://www.npmjs.com/package/in-memoria)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Discord](https://img.shields.io/discord/1431193342200516630?color=7289da\u0026label=Discord\u0026logo=discord\u0026logoColor=white)](https://discord.gg/6mGsM4qkYm)\n\n**Giving AI coding assistants a memory that actually persists.**\n\n## Quick Demo\n\n[![asciicast](https://asciinema.org/a/ZyD2bAZs1cURnqoFc3VHXemJx.svg)](https://asciinema.org/a/ZyD2bAZs1cURnqoFc3VHXemJx)\n\n_Watch In Memoria in action: learning a codebase, providing instant context, and routing features to files._\n\n---\n\n## The Problem: Session Amnesia\n\nYou know the drill. You fire up Claude, Copilot, or Cursor to help with your codebase. You explain your architecture. You describe your patterns. You outline your conventions. The AI gets it, helps you out, and everything\u0027s great.\n\nThen you close the window.\n\nNext session? Complete amnesia. You\u0027re explaining the same architectural decisions again. The same naming conventions. The same \"no, we don\u0027t use classes here, we use functional composition\" for the fifteenth time.\n\n**Every AI coding session starts from scratch.**\n\nThis isn\u0027t just annoying, it\u0027s inefficient. These tools re-analyze your codebase on every interaction, burning tokens and time. They give generic suggestions that don\u0027t match your style. They have no memory of what worked last time, what you rejected, or why.\n\n## The Solution: Persistent Intelligence\n\nIn Memoria is an MCP server that learns from your actual codebase and remembers across sessions. It builds persistent intelligence about your code (patterns, architecture, conventions, decisions) that AI assistants can query through the Model Context Protocol.\n\nThink of it as giving your AI pair programmer a notepad that doesn\u0027t get wiped clean every time you restart the session.\n\n**Current version: 0.6.0** - [See what\u0027s changed](CHANGELOG.md)\n\n### What It Does\n\n- **Learns your patterns** - Analyzes your code to understand naming conventions, architectural choices, and structural preferences\n- **Instant project context** - Provides tech stack, entry points, and architecture in \u003c200 tokens (no re-analysis needed)\n- **Smart file routing** - Routes vague requests like \"add password reset\" directly to relevant files\n- **Semantic search** - Finds code by meaning, not just keywords\n- **Work memory** - Tracks current tasks and architectural decisions across sessions\n- **Pattern prediction** - Suggests how you\u0027d solve similar problems based on your history\n\n### Example Workflow\n\n```bash\n# First time: Learn your codebase\nnpx in-memoria learn ./my-project\n\n# Start the MCP server\nnpx in-memoria server\n\n# Now in Claude/Copilot:\nYou: \"Add password reset functionality\"\nAI: *queries In Memoria*\n    \"Based on your auth patterns in src/auth/login.ts, I\u0027ll use your\n     established JWT middleware pattern and follow your Result\u003cT\u003e\n     error handling convention...\"\n\n# Next session (days later):\nYou: \"Where did we put the password reset code?\"\nAI: *queries In Memoria*\n    \"In src/auth/password-reset.ts, following the pattern we\n     established in our last session...\"\n```\n\nNo re-explaining. No generic suggestions. Just continuous, context-aware assistance.\n\n## Quick Start\n\n### Installation\n\n```bash\n# Install globally\nnpm install -g in-memoria\n\n# Or use directly with npx",
        "signals":  {
                        "mentions_local":  true,
                        "mentions_cloud":  false,
                        "mentions_api_key":  false,
                        "mentions_obsidian":  false,
                        "mentions_vector":  true,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  true,
                        "storage_sqlite":  true,
                        "storage_db":  false
                    }
    },
    {
        "name":  "pinecone-io/assistant-mcp",
        "url":  "https://github.com/pinecone-io/assistant-mcp",
        "kind":  "github",
        "list_desc":  "Connects to your Pinecone Assistant and gives the agent context from its knowledge engine.",
        "repo":  "pinecone-io/assistant-mcp",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/pinecone-io/assistant-mcp/main/README.md",
        "snippet":  "# Pinecone Assistant MCP Server\n\nAn MCP server implementation for retrieving information from Pinecone Assistant.\n\n## Features\n\n- Retrieves information from Pinecone Assistant\n- Supports multiple results retrieval with a configurable number of results\n\n## Prerequisites\n\n- Docker installed on your system\n- Pinecone API key - obtain from the [Pinecone Console](https://app.pinecone.io)\n- Pinecone Assistant API host - after creating an Assistant (e.g. in Pinecone Console), you can find the host in the Assistant details page\n\n## Building with Docker\n\nTo build the Docker image:\n\n```sh\ndocker build -t pinecone/assistant-mcp .\n```\n\n## Running with Docker\n\nRun the server with your Pinecone API key:\n\n```sh\ndocker run -i --rm \\\n  -e PINECONE_API_KEY=\u003cYOUR_PINECONE_API_KEY_HERE\u003e \\\n  -e PINECONE_ASSISTANT_HOST=\u003cYOUR_PINECONE_ASSISTANT_HOST_HERE\u003e \\\n  pinecone/assistant-mcp\n```\n\n### Environment Variables\n\n- `PINECONE_API_KEY` (required): Your Pinecone API key\n- `PINECONE_ASSISTANT_HOST` (optional): Pinecone Assistant API host (default: https://prod-1-data.ke.pinecone.io)\n- `LOG_LEVEL` (optional): Logging level (default: info)\n\n## Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"pinecone-assistant\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \n        \"-i\", \n        \"--rm\", \n        \"-e\", \n        \"PINECONE_API_KEY\", \n        \"-e\", \n        \"PINECONE_ASSISTANT_HOST\", \n        \"pinecone/assistant-mcp\"\n      ],\n      \"env\": {\n        \"PINECONE_API_KEY\": \"\u003cYOUR_PINECONE_API_KEY_HERE\u003e\",\n        \"PINECONE_ASSISTANT_HOST\": \"\u003cYOUR_PINECONE_ASSISTANT_HOST_HERE\u003e\"\n      }\n    }\n  }\n}\n```\n\n## Building from Source\n\nIf you prefer to build from source without Docker:\n\n1. Make sure you have Rust installed (https://rustup.rs/)\n2. Clone this repository\n3. Run `cargo build --release`\n4. The binary will be available at `target/release/assistant-mcp`\n\n### Testing with the inspector\n```sh\nexport PINECONE_API_KEY=\u003cYOUR_PINECONE_API_KEY_HERE\u003e",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  false,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  false,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "mattjoyce/mcp-persona-sessions",
        "url":  "https://github.com/mattjoyce/mcp-persona-sessions",
        "kind":  "github",
        "list_desc":  "Enable AI assistants to conduct structured, persona-driven sessions including interview preparation, personal reflection, and coaching conversations. Built-in timer management and performance evaluation tools.",
        "repo":  "mattjoyce/mcp-persona-sessions",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/mattjoyce/mcp-persona-sessions/master/README.md",
        "snippet":  "# mcp-persona-sessions\n\nA Model Context Protocol (MCP) server that enables AI assistants to conduct structured, persona-driven sessions including interview preparation, personal reflection, and coaching conversations.\n\n## Overview\n\nTransform your AI interactions with realistic persona-driven sessions. Originally designed for mock interview preparation (practice with your \"new boss\" before that important meeting), this MCP server has evolved into a flexible framework for guided conversations of all kinds.\n\nWhether you\u0027re preparing for a crucial presentation, seeking structured self-reflection, or wanting to practice difficult conversations in a safe environment, this server provides the framework for meaningful, guided dialogue.\n\n## Key Features\n\n- 🎭 **Persona-Driven Sessions**: Load detailed persona profiles that completely transform AI behavior and expertise\n- ⏱️ **Built-in Timer Management**: Track session duration with start, stop, and status checking\n- 📋 **Structured Frameworks**: Pre-defined session templates with clear goals and outcomes  \n- 🔄 **Adaptive Flow**: Sessions that respond to what emerges naturally in conversation\n- 📊 **Session Evaluation**: Get detailed feedback on performance and communication effectiveness\n- 🔒 **Secure Operation**: Safe file handling with path validation and error handling\n\n## Session Types\n\n### Meeting Preparation\nPractice conversations with realistic personas:\n- **Healthcare CIO**: Technical discussions with mission-driven leadership perspective\n- **Board Members**: High-level strategic conversations\n- **Team Leaders**: Collaborative planning and decision-making sessions\n\n### Personal Reflection\nAdaptive journaling sessions that flow between:\n- Daily experience processing\n- Creative exploration and inspiration\n- Gratitude practice and appreciation\n- Life pattern recognition and growth planning\n\n### Custom Sessions\nCreate your own personas and session frameworks for specific needs.\n\n## Quick Start\n\n### Prerequisites\n- Python 3.8+\n- MCP-compatible AI assistant (Claude Desktop, etc.)\n\n### Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/mattjoyce/mcp-persona-sessions.git\ncd mcp-persona-sessions\n```\n\n2. Install dependencies:\n```bash\npip install fastmcp\n```\n\n3. Set up configuration:\n```bash\ncp config.yaml.example config.yaml\n# Edit config.yaml with your preferences\n```\n\n### MCP Client Configuration\n\nAdd to your MCP client configuration (e.g., Claude Desktop):\n\n```json\n{\n  \"mcpServers\": {\n    \"persona-sessions\": {\n      \"command\": \"/path/to/venv/python\",\n      \"args\": [\"/path/to/mcp-persona-sessions/mcp-persona-sessions.py\"],\n      \"cwd\": \"/path/to/mcp-persona-sessions\"\n    }\n  }\n}\n```\n\n## Usage\n",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  false,
                        "mentions_api_key":  false,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "ragieai/mcp-server",
        "url":  "https://github.com/ragieai/ragie-mcp-server",
        "kind":  "github",
        "list_desc":  "Retrieve context from your [Ragie](https://www.ragie.ai) (RAG) knowledge base connected to integrations like Google Drive, Notion, JIRA and more.",
        "repo":  "ragieai/ragie-mcp-server",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/ragieai/ragie-mcp-server/main/README.md",
        "snippet":  "![image](https://github.com/user-attachments/assets/75e80f87-f39e-4f10-8c97-bbc848bbed82)\n\n\n# Ragie Model Context Protocol Server\n\nA Model Context Protocol (MCP) server that provides access to Ragie\u0027s knowledge base retrieval capabilities.\n\n## Description\n\nThis server implements the Model Context Protocol to enable AI models to retrieve information from a Ragie knowledge base. It provides a single tool called \"retrieve\" that allows querying the knowledge base for relevant information.\n\n## Prerequisites\n\n- Node.js \u003e= 18\n- A Ragie API key\n\n## Installation\n\nThe server requires the following environment variable:\n\n- `RAGIE_API_KEY` (required): Your Ragie API authentication key\n\nThe server will start and listen on stdio for MCP protocol messages.\n\nInstall and run the server with npx:\n\n```bash\nRAGIE_API_KEY=your_api_key npx @ragieai/mcp-server\n```\n\n### Command Line Options\n\nThe server supports the following command line options:\n\n- `--description, -d \u003ctext\u003e`: Override the default tool description with custom text\n- `--partition, -p \u003cid\u003e`: Specify the Ragie partition ID to query\n\nExamples:\n\n```bash\n# With custom description\nRAGIE_API_KEY=your_api_key npx @ragieai/mcp-server --description \"Search the company knowledge base for information\"\n\n# With partition specified\nRAGIE_API_KEY=your_api_key npx @ragieai/mcp-server --partition your_partition_id\n\n# Using both options\nRAGIE_API_KEY=your_api_key npx @ragieai/mcp-server --description \"Search the company knowledge base\" --partition your_partition_id\n```\n\n## Cursor Configuration\n\nTo use this MCP server with Cursor:\n\n### Option 1: Create an MCP configuration file\n\n1. Save a file called `mcp.json`\n\n* **For tools specific to a project**, create a `.cursor/mcp.json` file in your project directory. This allows you to define MCP servers that are only available within that specific project.\n* **For tools that you want to use across all projects**, create a `~/.cursor/mcp.json` file in your home directory. This makes MCP servers available in all your Cursor workspaces.\n\nExample `mcp.json`:\n```json\n{\n  \"mcpServers\": {\n    \"ragie\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@ragieai/mcp-server\",\n        \"--partition\",\n        \"optional_partition_id\"\n      ],\n      \"env\": {\n        \"RAGIE_API_KEY\": \"your_api_key\"\n      }\n    }\n  }\n}\n```",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  false,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  false,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "redleaves/context-keeper",
        "url":  "https://github.com/redleaves/context-keeper",
        "kind":  "github",
        "list_desc":  "LLM-driven context and memory management with wide-recall + precise-reranking RAG architecture. Features multi-dimensional retrieval (vector/timeline/knowledge graph), short/long-term memory, and complete MCP support (HTTP/WebSocket/SSE).",
        "repo":  "redleaves/context-keeper",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/redleaves/context-keeper/main/README.md",
        "snippet":  "\u003cdiv align=\"center\"\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"docs/img/elephant.png\" alt=\"Context-Keeper Logo\" width=\"120\"/\u003e\n\u003c/p\u003e\n\n# Context-Keeper\n\n**LLM-Driven Intelligent Memory \u0026 Context Management System**\n\n*Redefining AI Assistant Memory Boundaries - Making Every Conversation Meaningful*\n\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://github.com/redleaves/context-keeper\"\u003e\u003cimg src=\"https://img.shields.io/github/stars/redleaves/context-keeper?style=for-the-badge\u0026logo=github\u0026color=ff69b4\" alt=\"GitHub Stars\"/\u003e\u003c/a\u003e\n  \u003ca href=\"LICENSE\"\u003e\u003cimg src=\"https://img.shields.io/badge/License-MIT-blue.svg?style=for-the-badge\" alt=\"License\"/\u003e\u003c/a\u003e\n  \u003ca href=\"https://golang.org/\"\u003e\u003cimg src=\"https://img.shields.io/badge/Go-1.21+-00ADD8?style=for-the-badge\u0026logo=go\" alt=\"Go Version\"/\u003e\u003c/a\u003e\n  \u003ca href=\"https://github.com/modelcontextprotocol\"\u003e\u003cimg src=\"https://img.shields.io/badge/MCP-Compatible-green?style=for-the-badge\" alt=\"MCP Protocol\"/\u003e\u003c/a\u003e\n\u003c/p\u003e\n\n\u003ch3 align=\"center\"\u003e\n  English | \u003ca href=\"README-zh-CN.md\"\u003e简体中文\u003c/a\u003e\n\u003c/h3\u003e\n\n\u003c/div\u003e\n\n---\n\n## 📋 **Table of Contents**\n\n- [🎯 Why Context-Keeper?](#1-ai-development-challenges-when-intelligent-tools-meet-memory-gaps)\n- [🎯 Core Features](#2-core-features)\n- [🏗️ Architecture Design](#3-architecture-design)\n- [📖 Deployment \u0026 Integration](#4-deployment--integration)\n- [🗺️ Product Roadmap](#5-product-roadmap)\n- [🤝 Contributing](#6-contributing-guide)\n\n---\n\n## 1. AI Development Challenges: When Intelligent Tools Meet Memory Gaps\n\n\u003e **\"Do you remember the microservices architecture we discussed yesterday?\"** → \"Sorry, I don\u0027t remember...\" → 😤\n\n### 📊 **Four-Dimensional Pain Points: Which One Are You?**\n\n\u003cdiv align=\"center\"\u003e\n\n|  | 👤 **Individual Developer** | 👥 **Team Leader** | 🏗️ **Project Manager** | 🏢 **Enterprise Executive** |\n|------|-----------------|----------------|----------------|----------------|\n| **💔 Core Pain Points** | 🔄 **Daily Repetition**: Explaining project context to AI\u003cbr/\u003e🧠 **Context Loss**: AI can\u0027t understand development intent\u003cbr/\u003e🌀 **Redundant Work**: Solving similar problems repeatedly | 📚 **Knowledge Gap**: Senior experience can\u0027t be inherited\u003cbr/\u003e💬 **High Communication Cost**: Repeatedly explaining same issues\u003cbr/\u003e🚫 **Decision Delays**: Lack of historical context reference | 🔧 **Technical Debt**: Unknown reasons for historical decisions\u003cbr/\u003e⏱️ **Project Delays**: Long onboarding cycle for new members\u003cbr/\u003e📋 **Documentation Lag**: Code and docs out of sync | 💸 **Talent Loss**: Core knowledge leaves with personnel\u003cbr/\u003e📈 **ROI Decline**: Cross-project best practices hard to reuse\u003cbr/\u003e🎯 **Competitive Disadvantage**: Innovation speed slowed down |\n| **⚡ Direct Impact** | **🔥30% Development Time Wasted** | **📉Team Efficiency Down 40%** | **💰Project Cost 2x Over Budget** | **⏰Talent Training Cost 6-12 Months** |\n\n\u003c/div\u003e\n\n### 🔥 **Industry Status Data**\n\n- 📊 **50% of developers** repeat project context explanations to AI assistants daily\n- 💰 **Average Cost**: Replacing a senior engineer takes 6-12 months\n- ⏱️ **Time Loss**: New members need 3-6 months to fully understand complex projects\n- 🔄 **Repetitive Work**: 30-40% of technical issues in teams are repetitive\n\n**Core Problem**: AI tools lack continuous memory capabilities and cannot form intelligent knowledge accumulation and inheritance systems. Facing these challenges, we need not another memory tool, but a truly intelligent brain that understands developer intent.\n\n🚀 **Context-Keeper: Breaking Traditional Boundaries with Intelligent Solutions**\n\n---\n\n## 2. Core Features\n\n```mermaid\n%%{init: {\u0027theme\u0027:\u0027base\u0027, \u0027themeVariables\u0027: {\u0027fontSize\u0027:\u002716px\u0027, \u0027fontFamily\u0027:\u0027Arial, sans-serif\u0027}}}%%\ngraph LR\n    subgraph Stage1[\"🔍 Multi-Dimensional Wide Recall\u003cbr/\u003e(High Coverage)\"]\n        A1(\"Semantic Retrieval\u003cbr/\u003eTOP-50\") \n        A2(\"Timeline Retrieval\u003cbr/\u003eTOP-30\")\n        A3(\"Knowledge Graph\u003cbr/\u003eTOP-20\")\n        A1 --\u003e A4(\"Candidate Set\u003cbr/\u003e~100 items\")\n        A2 --\u003e A4\n        A3 --\u003e A4\n    end\n    ",
        "signals":  {
                        "mentions_local":  true,
                        "mentions_cloud":  true,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  false,
                        "mentions_vector":  true,
                        "mentions_graph":  true,
                        "mentions_web":  false,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  true
                    }
    },
    {
        "name":  "shinpr/mcp-local-rag",
        "url":  "https://github.com/shinpr/mcp-local-rag",
        "kind":  "github",
        "list_desc":  "Privacy-first document search server running entirely locally. Supports semantic search over PDFs, DOCX, TXT, and Markdown files with LanceDB vector storage and local embeddings - no API keys or cloud services required.",
        "repo":  "shinpr/mcp-local-rag",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/shinpr/mcp-local-rag/main/README.md",
        "snippet":  "# MCP Local RAG\n\n[![npm version](https://img.shields.io/npm/v/mcp-local-rag.svg)](https://www.npmjs.com/package/mcp-local-rag)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue.svg?logo=typescript\u0026logoColor=white)](https://www.typescriptlang.org/)\n[![MCP Registry](https://img.shields.io/badge/MCP-Registry-green.svg)](https://registry.modelcontextprotocol.io/)\n\nLocal RAG for developers using MCP.\nSemantic search with keyword boost for exact technical terms — fully private, zero setup.\n\n## Features\n\n- **Semantic search with keyword boost**\n  Vector search first, then keyword matching boosts exact matches. Terms like `useEffect`, error codes, and class names rank higher—not just semantically guessed.\n\n- **Smart semantic chunking**\n  Chunks documents by meaning, not character count. Uses embedding similarity to find natural topic boundaries—keeping related content together and splitting where topics change.\n\n- **Quality-first result filtering**\n  Groups results by relevance gaps instead of arbitrary top-K cutoffs. Get fewer but more trustworthy chunks.\n\n- **Runs entirely locally**\n  No API keys, no cloud, no data leaving your machine. Works fully offline after the first model download.\n\n- **Zero-friction setup**\n  One `npx` command. No Docker, no Python, no servers to manage. Designed for Cursor, Codex, and Claude Code via MCP.\n\n## Quick Start\n\nSet `BASE_DIR` to the folder you want to search. Documents must live under it.\n\nAdd the MCP server to your AI coding tool:\n\n**For Cursor** — Add to `~/.cursor/mcp.json`:\n```json\n{\n  \"mcpServers\": {\n    \"local-rag\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-local-rag\"],\n      \"env\": {\n        \"BASE_DIR\": \"/path/to/your/documents\"\n      }\n    }\n  }\n}\n```\n\n**For Codex** — Add to `~/.codex/config.toml`:\n```toml\n[mcp_servers.local-rag]\ncommand = \"npx\"\nargs = [\"-y\", \"mcp-local-rag\"]\n\n[mcp_servers.local-rag.env]\nBASE_DIR = \"/path/to/your/documents\"\n```\n\n**For Claude Code** — Run this command:\n```bash\nclaude mcp add local-rag --scope user --env BASE_DIR=/path/to/your/documents -- npx -y mcp-local-rag\n```\n\nRestart your tool, then start using it:\n\n```\nYou: \"Ingest api-spec.pdf\"\nAssistant: Successfully ingested api-spec.pdf (47 chunks created)\n\nYou: \"What does the API documentation say about authentication?\"\nAssistant: Based on the documentation, authentication uses OAuth 2.0 with JWT tokens.\n          The flow is described in section 3.2...\n```\n\nThat\u0027s it. No installation, no Docker, no complex setup.\n\n## Why This Exists\n\nYou want AI to search your documents—technical specs, research papers, internal docs. But most solutions send your files to external APIs.\n",
        "signals":  {
                        "mentions_local":  true,
                        "mentions_cloud":  true,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  false,
                        "mentions_vector":  true,
                        "mentions_graph":  false,
                        "mentions_web":  true,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "smith-and-web/obsidian-mcp-server",
        "url":  "https://github.com/smith-and-web/obsidian-mcp-server",
        "kind":  "github",
        "list_desc":  "SSE-enabled MCP server for remote Obsidian vault management with 29 tools for notes, directories, frontmatter, tags, search, and link operations. Docker-ready with health monitoring.",
        "repo":  "smith-and-web/obsidian-mcp-server",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/smith-and-web/obsidian-mcp-server/main/README.md",
        "snippet":  "# Obsidian MCP Server\n\n[![CI](https://github.com/smith-and-web/obsidian-mcp-server/actions/workflows/ci.yml/badge.svg)](https://github.com/smith-and-web/obsidian-mcp-server/actions/workflows/ci.yml)\n[![npm](https://img.shields.io/npm/v/@smith-and-web/obsidian-mcp-server)](https://www.npmjs.com/package/@smith-and-web/obsidian-mcp-server)\n[![Docker](https://img.shields.io/badge/ghcr.io-latest-blue)](https://ghcr.io/smith-and-web/obsidian-mcp-server)\n[![License](https://img.shields.io/github/license/smith-and-web/obsidian-mcp-server)](LICENSE)\n[![Node.js](https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen)](https://nodejs.org)\n[![MCP](https://img.shields.io/badge/MCP-compatible-blue)](https://modelcontextprotocol.io)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.x-blue)](https://www.typescriptlang.org/)\n\nA [Model Context Protocol (MCP)](https://modelcontextprotocol.io) server that enables AI assistants like Claude to interact with your Obsidian vault. Access your notes, create content, manage tags, and search your knowledge base through natural conversation.\n\n## Features\n\n### Note Management\n- **CRUD Operations**: Create, read, update, and delete notes (with safety confirmation)\n- **Write Modes**: Overwrite, append, or prepend content\n- **Batch Reading**: Read multiple notes in a single request\n- **File Info**: Get metadata without reading content (efficient for large vaults)\n- **Move/Duplicate**: Reorganize your vault structure\n- **Section Operations**: Read, append, or replace specific sections by heading\n\n### Frontmatter \u0026 Tags\n- **Frontmatter Parsing**: Get/set YAML frontmatter as structured JSON (powered by gray-matter)\n- **Tag Management**: Add/remove tags (frontmatter or inline)\n- **Tag Auditing**: Find notes missing required tags\n- **Tag Search**: List all tags with usage counts\n\n### Search \u0026 Links\n- **Full-Text Search**: Search content and filenames with context\n- **Backlinks**: Find all notes linking to a specific note\n- **Broken Links**: Detect wiki-links that don\u0027t resolve\n- **Find \u0026 Replace**: Bulk text replacement with regex support\n\n### Directory Operations\n- **Create/Delete/Rename**: Full directory management\n- **List Contents**: Browse vault structure\n\n### Performance\n- **Token Optimization**: Optional compact response mode (40-60% smaller responses)\n- **Efficient Scanning**: Get file info without reading content\n- **SSE Transport**: Remote access without local installation\n\n## Quick Start\n\n### npx (Quickest)\n\nRun directly without installation:\n\n```bash\nVAULT_PATH=/path/to/your/vault npx @smith-and-web/obsidian-mcp-server\n```\n\nWith options:\n\n```bash\nVAULT_PATH=/path/to/vault PORT=3001 COMPACT_RESPONSES=true npx @smith-and-web/obsidian-mcp-server\n```\n\n### Docker (Recommended for Production)\n\n**Using the pre-built image from GitHub Container Registry:**\n\n```bash\ndocker run -d \\\n  --name obsidian-mcp \\\n  -v /path/to/your/vault:/vault:rw \\\n  -p 3001:3000 \\\n  -e VAULT_PATH=/vault \\\n  ghcr.io/smith-and-web/obsidian-mcp-server:latest\n```\n\n**Or with Docker Compose:**\n\n1. **Create a `docker-compose.yml`:**\n   ```yaml\n   version: \u00273.8\u0027\n   services:\n     obsidian-mcp:\n       image: ghcr.io/smith-and-web/obsidian-mcp-server:latest",
        "signals":  {
                        "mentions_local":  true,
                        "mentions_cloud":  false,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  true,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "Smart-AI-Memory/empathy-framework",
        "url":  "https://github.com/Smart-AI-Memory/empathy-framework",
        "kind":  "github",
        "list_desc":  "Five-level AI collaboration system with persistent memory and anticipatory capabilities. MCP-native integration for Claude and other LLMs with local-first architecture via MemDocs.",
        "repo":  "Smart-AI-Memory/empathy-framework",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/Smart-AI-Memory/empathy-framework/main/README.md",
        "snippet":  "# Empathy Framework\n\n**The AI collaboration framework with breakthrough meta-orchestration - agents that compose themselves.**\n\n🎭 **v4.0: The Meta-Orchestration Era** - Dynamic agent teams, intelligent composition, self-learning systems.\n\n[![PyPI](https://img.shields.io/pypi/v/empathy-framework)](https://pypi.org/project/empathy-framework/)\n[![Tests](https://img.shields.io/badge/tests-6%2C038%20passing-brightgreen)](https://github.com/Smart-AI-Memory/empathy-framework/actions)\n[![Coverage](https://img.shields.io/badge/coverage-68%25-yellow)](https://github.com/Smart-AI-Memory/empathy-framework)\n[![License](https://img.shields.io/badge/license-Fair%20Source%200.9-blue)](LICENSE)\n[![Python](https://img.shields.io/badge/python-3.10+-blue)](https://www.python.org)\n[![Security](https://img.shields.io/badge/security-hardened-green)](https://github.com/Smart-AI-Memory/empathy-framework/blob/main/SECURITY.md)\n\n```bash\npip install empathy-framework[developer]  # Lightweight for individual developers\n```\n\n## What\u0027s New in v4.0.0 🎭 **PARADIGM SHIFT**\n\n### **Meta-Orchestration: AI Agents That Compose Themselves**\n\n**The breakthrough:** Instead of manually wiring agent workflows, v4.0 introduces a meta-orchestration system that analyzes tasks, selects optimal agent teams, chooses composition patterns, and learns from outcomes.\n\n**What this means:**\n\n- 🧠 **Automatic task analysis** → Determines complexity, domain, required capabilities\n- 🤝 **Dynamic team composition** → Selects optimal agents from 7 pre-built templates\n- 📐 **Intelligent strategy selection** → Chooses from 6 composition patterns (Sequential, Parallel, Debate, Teaching, Refinement, Adaptive)\n- 📚 **Self-learning** → Saves successful compositions and improves over time\n- ⚡ **Production-ready workflows** → Release Prep (parallel validation), Test Coverage Boost (sequential improvement)\n\n### Quick Start\n\n**Release preparation with 4 parallel agents:**\n\n```bash\nempathy orchestrate release-prep\n```\n\nAutomatically runs:\n\n- **Security Auditor** (vulnerability scan)\n- **Test Coverage Analyzer** (gap analysis)\n- **Code Quality Reviewer** (best practices)\n- **Documentation Writer** (completeness check)\n\n**Boost test coverage to 90%:**\n\n```bash\nempathy orchestrate test-coverage --target 90\n```\n\nSequential workflow:\n\n1. **Coverage Analyzer** → Identify gaps\n2. **Test Generator** → Create tests\n3. **Test Validator** → Verify coverage\n\n### Python API\n\n```python\nfrom empathy_os.workflows.orchestrated_release_prep import (\n    OrchestratedReleasePrepWorkflow\n)\n\n# Create workflow with custom quality gates\nworkflow = OrchestratedReleasePrepWorkflow(\n    quality_gates={\n        \"min_coverage\": 90.0,\n        \"max_critical_issues\": 0,\n    }\n)\n\n# Execute\nreport = await workflow.execute(path=\".\")\n\nif report.approved:\n    print(f\"✅ Release approved! (confidence: {report.confidence})\")\nelse:\n    for blocker in report.blockers:",
        "signals":  {
                        "mentions_local":  true,
                        "mentions_cloud":  true,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  false,
                        "mentions_vector":  true,
                        "mentions_graph":  true,
                        "mentions_web":  false,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  true
                    }
    },
    {
        "name":  "TechDocsStudio/biel-mcp",
        "url":  "https://github.com/TechDocsStudio/biel-mcp",
        "kind":  "github",
        "list_desc":  "Let AI tools like Cursor, VS Code, or Claude Desktop answer questions using your product docs. Biel.ai provides the RAG system and MCP server.",
        "repo":  "TechDocsStudio/biel-mcp",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/TechDocsStudio/biel-mcp/main/README.md",
        "snippet":  "\u003cdiv align=\"center\"\u003e\n  \u003cpicture\u003e\n    \u003csource media=\"(prefers-color-scheme: dark)\" srcset=\"./logo-dark..jpg\" /\u003e\n    \u003cimg alt=\"Biel.ai\" src=\"./logo.jpg\" /\u003e\n  \u003c/picture\u003e\n  \u003ch1\u003eBiel.ai MCP Server\u003c/h1\u003e\n  \u003ch3\u003eConnect your IDE to your product docs\u003c/h3\u003e\n\u003c/div\u003e\n\n\nGive AI tools like Cursor, VS Code, and Claude Desktop access to your company\u0027s product knowledge through the [Biel.ai platform](https://biel.ai).\n\nBiel.ai provides a hosted Retrieval-Augmented Generation (RAG) layer that makes your documentation searchable and useful to AI tools. This enables smarter completions, accurate technical answers, and context-aware suggestions—directly in your IDE or chat environment.\n\n![Demo](./demo.png)\n\nWhen AI tools can read your product documentation, they become **significantly** more helpful—generating more accurate code completions, answering technical questions with context, and guiding developers with real-time product knowledge.\n\n\n\u003e **Note:** Requires a Biel.ai account and project setup. **[Start your free 15-day trial](https://app.biel.ai/accounts/signup/)**.\n\n\u003ch3\u003e\u003ca href=\"https://docs.biel.ai/integrations/mcp-server?utm_source=github\u0026utm_medium=referral\u0026utm_campaign=readme\"\u003eSee quickstart instructions →\u003c/a\u003e\u003c/h3\u003e\n\n## Getting started\n\n### 1. Get your MCP configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"biel-ai\": {\n      \"description\": \"Query your product\u0027s documentation, APIs, and knowledge base.\",\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"https://mcp.biel.ai/sse?project_slug=YOUR_PROJECT_SLUG\u0026domain=https://your-docs-domain.com\"\n      ]\n    }\n  }\n}\n```\n\n**Required:** `project_slug` and `domain`  \n**Optional:** `api_key` (only needed for private projects)\n\n### 2. Add to your AI tool\n\n* **Cursor**: **Settings** → **Tools \u0026 Integrations* → **New MCP server**.\n* **Claude Desktop**: Edit `claude_desktop_config.json`  \n* **VS Code**: Install **MCP extension**.\n\n### 3. Start asking questions\n\n```\nCan you check in biel_ai what the auth headers are for the /users endpoint?\n```\n\n## Self-hosting (Optional)\n\nFor advanced users who prefer to run their own MCP server instance:\n\n### Local development\n```bash\n# Clone and run locally\ngit clone https://github.com/techdocsStudio/biel-mcp\ncd biel-mcp\npip install -r requirements.txt\npython biel_mcp_server.py\n```\n\n### Docker deployment\n```bash\n# Docker Compose (recommended)\ndocker-compose up -d --build\n\n# Or Docker directly\ndocker build -t biel-mcp .\ndocker run -d -p 7832:7832 biel-mcp\n```\n",
        "signals":  {
                        "mentions_local":  true,
                        "mentions_cloud":  true,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  false,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "topoteretes/cognee",
        "url":  "https://github.com/topoteretes/cognee/tree/dev/cognee-mcp",
        "kind":  "github",
        "list_desc":  "Memory manager for AI apps and Agents using various graph and vector stores and allowing ingestion from 30+ data sources",
        "repo":  "topoteretes/cognee",
        "branch":  "dev",
        "subdir":  "cognee-mcp",
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/topoteretes/cognee/dev/cognee-mcp/README.md",
        "snippet":  "\u003cdiv align=\"center\"\u003e\n  \u003ca href=\"https://github.com/topoteretes/cognee\"\u003e\n    \u003cimg src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png\" alt=\"Cognee Logo\" height=\"60\"\u003e\n  \u003c/a\u003e\n\n  \u003cbr /\u003e\n\n  cognee‑mcp - Run cognee’s memory engine as a Model Context Protocol server\n\n  \u003cp align=\"center\"\u003e\n  \u003ca href=\"https://www.youtube.com/watch?v=1bezuvLwJmw\u0026t=2s\"\u003eDemo\u003c/a\u003e\n  .\n  \u003ca href=\"https://cognee.ai\"\u003eLearn more\u003c/a\u003e\n  ·\n  \u003ca href=\"https://discord.gg/NQPKmU5CCg\"\u003eJoin Discord\u003c/a\u003e\n  ·\n  \u003ca href=\"https://www.reddit.com/r/AIMemory/\"\u003eJoin r/AIMemory\u003c/a\u003e\n  \u003c/p\u003e\n\n\n  [![GitHub forks](https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social\u0026label=Fork\u0026maxAge=2592000)](https://GitHub.com/topoteretes/cognee/network/)\n  [![GitHub stars](https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social\u0026label=Star\u0026maxAge=2592000)](https://GitHub.com/topoteretes/cognee/stargazers/)\n  [![GitHub commits](https://badgen.net/github/commits/topoteretes/cognee)](https://GitHub.com/topoteretes/cognee/commit/)\n  [![Github tag](https://badgen.net/github/tag/topoteretes/cognee)](https://github.com/topoteretes/cognee/tags/)\n  [![Downloads](https://static.pepy.tech/badge/cognee)](https://pepy.tech/project/cognee)\n  [![License](https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586\u0026colorB=000000)](https://github.com/topoteretes/cognee/blob/main/LICENSE)\n  [![Contributors](https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586\u0026colorB=000000)](https://github.com/topoteretes/cognee/graphs/contributors)\n\n\u003ca href=\"https://www.producthunt.com/posts/cognee?embed=true\u0026utm_source=badge-top-post-badge\u0026utm_medium=badge\u0026utm_souce=badge-cognee\" target=\"_blank\"\u003e\u003cimg src=\"https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=946346\u0026theme=light\u0026period=daily\u0026t=1744472480704\" alt=\"cognee - Memory\u0026#0032;for\u0026#0032;AI\u0026#0032;Agents\u0026#0032;\u0026#0032;in\u0026#0032;5\u0026#0032;lines\u0026#0032;of\u0026#0032;code | Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" /\u003e\u003c/a\u003e\n\n\u003ca href=\"https://trendshift.io/repositories/13955\" target=\"_blank\"\u003e\u003cimg src=\"https://trendshift.io/api/badge/repositories/13955\" alt=\"topoteretes%2Fcognee | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/\u003e\u003c/a\u003e\n\n\nBuild memory for Agents and query from any client that speaks MCP – in your terminal or IDE.\n\n\u003c/div\u003e\n\n## ✨ Features\n\n- Multiple transports – choose Streamable HTTP --transport http (recommended for web deployments), SSE --transport sse (real‑time streaming), or stdio (classic pipe, default)\n- **API Mode** – connect to an already running Cognee FastAPI server instead of using cognee directly (see [API Mode](#-api-mode) below)\n- Integrated logging – all actions written to a rotating file (see get_log_file_location()) and mirrored to console in dev\n- Local file ingestion – feed .md, source files, Cursor rule‑sets, etc. straight from disk\n- Background pipelines – long‑running cognify \u0026 codify jobs spawn off‑thread; check progress with status tools\n- Developer rules bootstrap – one call indexes .cursorrules, .cursor/rules, AGENT.md, and friends into the developer_rules nodeset\n- Prune \u0026 reset – wipe memory clean with a single prune call when you want to start fresh\n\nPlease refer to our documentation [here](https://docs.cognee.ai/how-to-guides/deployment/mcp) for further information.\n\n## 🚀 Quick Start\n\n1. Clone cognee repo\n    ```\n    git clone https://github.com/topoteretes/cognee.git\n    ```\n2. Navigate to cognee-mcp subdirectory\n    ```\n    cd cognee/cognee-mcp\n    ```\n3. Install uv if you don\u0027t have one\n    ```\n    pip install uv\n    ```\n4. Install all the dependencies you need for cognee mcp server with uv\n    ```\n    uv sync --dev --all-extras --reinstall\n    ```\n5. Activate the virtual environment in cognee mcp directory\n    ```\n    source .venv/bin/activate\n    ```\n6. Set up your OpenAI API key in .env for a quick setup with the default cognee configurations\n    ```\n    LLM_API_KEY=\"YOUR_OPENAI_API_KEY\"\n    ```\n7. Run cognee mcp server with stdio (default)\n    ```\n    python src/server.py\n    ```\n    or stream responses over SSE",
        "signals":  {
                        "mentions_local":  true,
                        "mentions_cloud":  false,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  false,
                        "mentions_vector":  true,
                        "mentions_graph":  true,
                        "mentions_web":  true,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  true
                    }
    },
    {
        "name":  "unibaseio/membase-mcp",
        "url":  "https://github.com/unibaseio/membase-mcp",
        "kind":  "github",
        "list_desc":  "Save and query your agent memory in distributed way by Membase",
        "repo":  "unibaseio/membase-mcp",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/unibaseio/membase-mcp/main/README.md",
        "snippet":  "# membase mcp server\n\n## Description\n\nMembase is the first decentralized memory layer for AI agents, powered by Unibase. It provides secure, persistent storage for conversation history, interaction records, and knowledge — ensuring agent continuity, personalization, and traceability.\n\nThe Membase-MCP Server enables seamless integration with the Membase protocol, allowing agents to upload and retrieve memory from the Unibase DA network for decentralized, verifiable storage.\n\n## Functions\n\nMessages or memoiries can be visit at: \u003chttps://testnet.hub.membase.io/\u003e\n\n- **get_conversation_id**: Get the current conversation id.\n- **switch_conversation**: Switch to a different conversation.\n- **save_message**: Save a message/memory into the current conversation.\n- **get_messages**: Get the last n messages from the current conversation.\n\n## Installation\n\n```shell\ngit clone https://github.com/unibaseio/membase-mcp.git\ncd membase-mcp\nuv run src/membase_mcp/server.py\n```\n\n## Environment variables\n\n- MEMBASE_ACCOUNT: your account to upload\n- MEMBASE_CONVERSATION_ID: your conversation id, should be unique, will preload its history\n- MEMBASE_ID: your instance id\n\n## Configuration on Claude/Windsurf/Cursor/Cline\n\n```json\n{\n  \"mcpServers\": {\n    \"membase\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"path/to/membase-mcp\",\n        \"run\", \n        \"src/membase_mcp/server.py\"\n      ],\n      \"env\": {\n        \"MEMBASE_ACCOUNT\": \"your account, 0x...\",\n        \"MEMBASE_CONVERSATION_ID\": \"your conversation id, should be unique\",\n        \"MEMBASE_ID\": \"your sub account, any string\"\n      }\n    }\n  }\n}\n```\n\n## Usage\n\ncall functions in llm chat\n\n- get conversation id and switch conversation\n\n![get conversation id and switch conversation](./asset/switch.png)\n\n- save message and get messages\n\n![save message and get messages](./asset/save.png)\n",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  false,
                        "mentions_api_key":  false,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  false,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "upstash/context7",
        "url":  "https://github.com/upstash/context7",
        "kind":  "github",
        "list_desc":  "Up-to-date code documentation for LLMs and AI code editors.",
        "repo":  "upstash/context7",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/upstash/context7/master/README.md",
        "snippet":  "![Cover](https://github.com/upstash/context7/blob/master/public/cover.png?raw=true)\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7\u0026config=eyJ1cmwiOiJodHRwczovL21jcC5jb250ZXh0Ny5jb20vbWNwIn0%3D)\n\n# Context7 MCP - Up-to-date Code Docs For Any Prompt\n\n[![Website](https://img.shields.io/badge/Website-context7.com-blue)](https://context7.com) [![smithery badge](https://smithery.ai/badge/@upstash/context7-mcp)](https://smithery.ai/server/@upstash/context7-mcp) [![NPM Version](https://img.shields.io/npm/v/%40upstash%2Fcontext7-mcp?color=red)](https://www.npmjs.com/package/@upstash/context7-mcp) [![MIT licensed](https://img.shields.io/npm/l/%40upstash%2Fcontext7-mcp)](./LICENSE)\n\n[![繁體中文](https://img.shields.io/badge/docs-繁體中文-yellow)](./i18n/README.zh-TW.md) [![简体中文](https://img.shields.io/badge/docs-简体中文-yellow)](./i18n/README.zh-CN.md) [![日本語](https://img.shields.io/badge/docs-日本語-b7003a)](./i18n/README.ja.md) [![한국어 문서](https://img.shields.io/badge/docs-한국어-green)](./i18n/README.ko.md) [![Documentación en Español](https://img.shields.io/badge/docs-Español-orange)](./i18n/README.es.md) [![Documentation en Français](https://img.shields.io/badge/docs-Français-blue)](./i18n/README.fr.md) [![Documentação em Português (Brasil)](\u003chttps://img.shields.io/badge/docs-Português%20(Brasil)-purple\u003e)](./i18n/README.pt-BR.md) [![Documentazione in italiano](https://img.shields.io/badge/docs-Italian-red)](./i18n/README.it.md) [![Dokumentasi Bahasa Indonesia](https://img.shields.io/badge/docs-Bahasa%20Indonesia-pink)](./i18n/README.id-ID.md) [![Dokumentation auf Deutsch](https://img.shields.io/badge/docs-Deutsch-darkgreen)](./i18n/README.de.md) [![Документация на русском языке](https://img.shields.io/badge/docs-Русский-darkblue)](./i18n/README.ru.md) [![Українська документація](https://img.shields.io/badge/docs-Українська-lightblue)](./i18n/README.uk.md) [![Türkçe Doküman](https://img.shields.io/badge/docs-Türkçe-blue)](./i18n/README.tr.md) [![Arabic Documentation](https://img.shields.io/badge/docs-Arabic-white)](./i18n/README.ar.md) [![Tiếng Việt](https://img.shields.io/badge/docs-Tiếng%20Việt-red)](./i18n/README.vi.md)\n\n## ❌ Without Context7\n\nLLMs rely on outdated or generic information about the libraries you use. You get:\n\n- ❌ Code examples are outdated and based on year-old training data\n- ❌ Hallucinated APIs that don\u0027t even exist\n- ❌ Generic answers for old package versions\n\n## ✅ With Context7\n\nContext7 MCP pulls up-to-date, version-specific documentation and code examples straight from the source — and places them directly into your prompt.\n\nAdd `use context7` to your prompt (or [set up a rule](#add-a-rule) to auto-invoke):\n\n```txt\nCreate a Next.js middleware that checks for a valid JWT in cookies\nand redirects unauthenticated users to `/login`. use context7\n```\n\n```txt\nConfigure a Cloudflare Worker script to cache\nJSON API responses for five minutes. use context7\n```\n\nContext7 fetches up-to-date code examples and documentation right into your LLM\u0027s context. No tab-switching, no hallucinated APIs that don\u0027t exist, no outdated code generation.\n\n## Installation\n\n\u003e [!NOTE]\n\u003e **API Key Recommended**: Get a free API key at [context7.com/dashboard](https://context7.com/dashboard) for higher rate limits.\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Cursor\u003c/b\u003e\u003c/summary\u003e\n\nGo to: `Settings` -\u003e `Cursor Settings` -\u003e `MCP` -\u003e `Add new global MCP server`\n\nPasting the following configuration into your Cursor `~/.cursor/mcp.json` file is the recommended approach. You may also install in a specific project by creating `.cursor/mcp.json` in your project folder. See [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol) for more info.\n\n\u003e Since Cursor 1.0, you can click the install button below for instant one-click installation.\n\n#### Cursor Remote Server Connection\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7\u0026config=eyJ1cmwiOiJodHRwczovL21jcC5jb250ZXh0Ny5jb20vbWNwIn0%3D)\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor Local Server Connection\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7\u0026config=eyJjb21tYW5kIjoibnB4IC15IEB1cHN0YXNoL2NvbnRleHQ3LW1jcCJ9)\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}",
        "signals":  {
                        "mentions_local":  true,
                        "mentions_cloud":  false,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  true,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  true
                    }
    },
    {
        "name":  "vectorize-io/hindsight",
        "url":  "https://github.com/vectorize-io/hindsight",
        "kind":  "github",
        "list_desc":  "Hindsight: Agent Memory That Works Like Human Memory - Built for AI Agents to manage Long Term Memory",
        "repo":  "vectorize-io/hindsight",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/vectorize-io/hindsight/main/README.md",
        "snippet":  "\u003cdiv align=\"center\"\u003e\n\n![Hindsight Banner](./hindsight-docs/static/img/banner.svg)\n\n[Documentation](https://hindsight.vectorize.io) • [Paper](https://arxiv.org/abs/2512.12818) • [Cookbook](https://hindsight.vectorize.io/cookbook) • [Hindsight Cloud](https://vectorize.io/hindsight/cloud)\n\n[![CI](https://github.com/vectorize-io/hindsight/actions/workflows/release.yml/badge.svg)](https://github.com/vectorize-io/hindsight/actions/workflows/release.yml)\n[![Slack Community](https://img.shields.io/badge/Slack-Join%20Community-4A154B?logo=slack)](https://join.slack.com/t/hindsight-space/shared_invite/zt-3klo21kua-VUCC_zHP5rIcXFB1_5yw6A)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/hindsight-api?label=PyPI)\n![NPM Downloads](https://img.shields.io/npm/dm/%40vectorize-io%2Fhindsight-client?logoColor=orange\u0026label=NPM\u0026color=blue\u0026link=https%3A%2F%2Fwww.npmjs.com%2Fpackage%2F%40vectorize-io%2Fhindsight-client)\n\n\n\u003c/div\u003e\n\n---\n\n## What is Hindsight?\n\nHindsight™ is an agent memory system built to create smarter agents that learn over time. It eliminates the shortcomings of alternative techniques such as RAG and knowledge graph and delivers state-of-the-art performance on long term memory tasks.\n\nHindsight addresses common challenges that have frustrated AI engineers building agents to automate tasks and assist users with conversational interfaces. Many of these challenges stem directly from a lack of memory.\n\n- **Inconsistency:** Agents complete tasks successfully one time, then fail when asked to complete the same task again. Memory gives the agent a mechanism to remember what worked and what didn\u0027t and to use that information to reduce errors and improve consistency.\n- **Hallucinations:** Long term memory can be seeded with external knowledge to ground agent behavior in reliable sources to augment training data.\n- **Cognitive Overload:** As workflows get complex, retrievals, tool calls, user messages and agent responses can grow to fill the context window leading to context rot. Short term memory optimization allows agents to reduce tokens and focus context by removing irrelevant details.\n\n## How is Hindsight Different From Other Memory Systems?\n\n![Overview](./hindsight-docs/static/img/hindsight-overview.webp)\n\nMost agent memory implementation rely on basic vector search or sometimes use a knowledge graph. Hindsight uses biomimetic data structures to organize agent memories in a way that is more like how human memory works:\n\n- **World:** Facts about the world (\"The stove gets hot\")\n- **Experiences:** Agent\u0027s own experiences (\"I touched the stove and it really hurt\")\n- **Opinion:** Beliefs with confidence scores (\"I shouldn\u0027t touch the stove again\" - .99 confidence)\n- **Observation:** Complex mental models derived by reflecting on facts and experiences (\"Curling irons, ovens, and fire are also hot. I shouldn\u0027t touch those either.\")\n\nMemories in Hindsight are stored in banks (i.e. memory banks). When memories are added to Hindsight, they are pushed into either the world facts or experiences memory pathway. They are then represented as a combination of entities, relationships, and time series with sparse/dense vector representations to aid in later recall.\n\nHindsight provides three simple methods to interact with the system:\n\n- **Retain:** Provide information to Hindsight that you want it to remember\n- **Recall:** Retrieve memories from Hindsight\n- **Reflect:** Reflect on memories and experiences to generate new observations and insights from existing memories.\n\n### Agent Memory That Learns\n\nA key goal of Hindsight is to build agent memory that enables agents to learn and improve over time. This is the role of the `reflect` operation which provides the agent to form broader opinions and observations over time.\n\nFor example, imagine a product support agent that is helping a user troubleshoot a problem. It uses a `search-documentation` tool it found on an MCP server. Later in the conversation, the agent discovers that the documentation returned from the tool wasn\u0027t for the product the user was asking about. The agent now has an experience in its memory bank. And just like humans, we want that agent to learn from its experience.\n\nAs the agent gains more experiences, `reflect` allows the agent to form observations about what worked, what didn\u0027t, and what to do differently the next time it encounters a similar task.\n\n---\n\n## Memory Performance \u0026 Accuracy\n\nHindsight has achieved state-of-the-art performance on the LongMemEval benchmark, widely used to assess memory system performance across a variety of conversational\nAI scenarios. The current reported performance of Hindsight and other agent memory solutions as of December 2025 is shown here:\n\n![Overview](./hindsight-docs/static/img/hindsight-bench.jpg)\n\nThe benchmark performance data for Hindsight and GPT-4o (full context) have been reproduced by research collaborators at the Virginia Tech [Sanghani Center for Artificial Intelligence and Data Analytics](https://sanghani.cs.vt.edu/) and The Washington Post. Other scores are self-reported by software vendors.\n\nA thorough examination of the techniques implemented in Hindsight and detailed breakdowns of benchmark performance are [available on arXiv](https://arxiv.org/abs/2512.12818). This research is currently being prepared for conference submission and the wider peer review process.\n\nThe benchmark results from this research can be inspected in our [visual benchmark explorer](https://hindsight-benchmarks.vercel.app). As additional improvements are made to Hindsight, new benchmark data will be available for review using this same tool.\n\n## Quick Start\n\n### Docker (recommended)\n\n```bash\nexport OPENAI_API_KEY=your-key\n\ndocker run --rm -it --pull always -p 8888:8888 -p 9999:9999 \\\n  -e HINDSIGHT_API_LLM_API_KEY=$OPENAI_API_KEY \\\n  -e HINDSIGHT_API_LLM_MODEL=o3-mini \\\n  -v $HOME/.hindsight-docker:/home/hindsight/.pg0 \\",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  true,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  false,
                        "mentions_vector":  true,
                        "mentions_graph":  true,
                        "mentions_web":  false,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "JamesANZ/memory-mcp",
        "url":  "https://github.com/JamesANZ/memory-mcp",
        "kind":  "github",
        "list_desc":  "An MCP server that stores and retrieves memories from multiple LLMs using MongoDB. Provides tools for saving, retrieving, adding, and clearing conversation memories with timestamps and LLM identification.",
        "repo":  "JamesANZ/memory-mcp",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/JamesANZ/memory-mcp/main/README.md",
        "snippet":  "# 🧠 Memory MCP Server\n\n\u003e **Persistent memory and context window caching for LLM conversations.** Save, retrieve, and manage memories with intelligent context archiving. MongoDB-backed storage.\n\nAn [MCP (Model Context Protocol)](https://modelcontextprotocol.io) server that provides memory management and context window caching for AI coding environments like Cursor and Claude Desktop.\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/JamesANZ/memory-mcp)](https://archestra.ai/mcp-catalog/jamesanz__memory-mcp)\n\n## Why Use Memory MCP?\n\n- 💾 **Persistent Storage** – MongoDB-backed memory that survives sessions\n- 🧠 **Context Caching** – Intelligent archiving and retrieval of conversation context\n- 🏷️ **Tag-based Search** – Organize and find memories by tags\n- 📊 **Relevance Scoring** – Automatically score archived content relevance\n- ⚡ **Easy Setup** – One-click install in Cursor or simple manual setup\n\n## Quick Start\n\nReady to add memory to your AI workflow? Install in seconds:\n\n**Install in Cursor (Recommended):**\n\n[🔗 Install in Cursor](cursor://anysphere.cursor-deeplink/mcp/install?name=memory-mcp\u0026config=eyJtZW1vcnktbWNwIjp7ImNvbW1hbmQiOiJucHgiLCJhcmdzIjpbIi15IiwiQGphbWVzYW56L21lbW9yeS1tY3AiXX19)\n\n**Or install manually:**\n\n```bash\nnpm install -g @jamesanz/memory-mcp\n# Or from source:\ngit clone https://github.com/JamesANZ/memory-mcp.git\ncd memory-mcp \u0026\u0026 npm install \u0026\u0026 npm run build\n```\n\n## Features\n\n### Basic Memory Tools\n- **`save-memories`** – Save memories to database (overwrites existing)\n- **`get-memories`** – Retrieve all stored memories\n- **`add-memories`** – Append new memories without overwriting\n- **`clear-memories`** – Remove all stored memories\n\n### Context Window Caching\n- **`archive-context`** – Archive conversation context with tags\n- **`retrieve-context`** – Retrieve relevant archived context\n- **`score-relevance`** – Score archived content relevance\n- **`create-summary`** – Create summaries of archived content\n- **`get-conversation-summaries`** – Get all summaries for a conversation\n- **`search-context-by-tags`** – Search archived content by tags\n\n## Installation\n\n### Cursor (One-Click)\n\nClick the install link above or use:\n\n```\ncursor://anysphere.cursor-deeplink/mcp/install?name=memory-mcp\u0026config=eyJtZW1vcnktbWNwIjp7ImNvbW1hbmQiOiJucHgiLCJhcmdzIjpbIi15IiwiQGphbWVzYW56L21lbW9yeS1tY3AiXX19\n```\n\n### Manual Installation\n\n**Requirements:** Node.js 18+, npm, MongoDB\n\n```bash\n# Clone and build\ngit clone https://github.com/JamesANZ/memory-mcp.git\ncd memory-mcp\nnpm install\nnpm run build\n\n# Set MongoDB connection string\nexport MONGODB_URI=\"mongodb://localhost:27017\"\n\n# Run server\nnpm start\n```\n\n### Claude Desktop\n\nAdd to `claude_desktop_config.json`:",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  false,
                        "mentions_api_key":  false,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  false,
                        "storage_sqlite":  false,
                        "storage_db":  true
                    }
    },
    {
        "name":  "JamesANZ/cross-llm-mcp",
        "url":  "https://github.com/JamesANZ/cross-llm-mcp",
        "kind":  "github",
        "list_desc":  "An MCP server that enables cross-LLM communication and memory sharing, allowing different AI models to collaborate and share context across conversations.",
        "repo":  "JamesANZ/cross-llm-mcp",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/JamesANZ/cross-llm-mcp/main/README.md",
        "snippet":  "# 🤖 Cross-LLM MCP Server\n\n\u003e **Access multiple LLM APIs from one place.** Call ChatGPT, Claude, DeepSeek, Gemini, Grok, Kimi, Perplexity, and Mistral with intelligent model selection, preferences, and prompt logging.\n\nAn [MCP (Model Context Protocol)](https://modelcontextprotocol.io) server that provides unified access to multiple Large Language Model APIs for AI coding environments like Cursor and Claude Desktop.\n\n[![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/JamesANZ/cross-llm-mcp)](https://archestra.ai/mcp-catalog/jamesanz__cross-llm-mcp)\n\n## Why Use Cross-LLM MCP?\n\n- 🌐 **8 LLM Providers** – ChatGPT, Claude, DeepSeek, Gemini, Grok, Kimi, Perplexity, Mistral\n- 🎯 **Smart Model Selection** – Tag-based preferences (coding, business, reasoning, math, creative, general)\n- 📊 **Prompt Logging** – Track all prompts with history, statistics, and analytics\n- 💰 **Cost Optimization** – Choose flagship or cheaper models based on preference\n- ⚡ **Easy Setup** – One-click install in Cursor or simple manual setup\n- 🔄 **Call All LLMs** – Get responses from all providers simultaneously\n\n## Quick Start\n\nReady to access multiple LLMs? Install in seconds:\n\n**Install in Cursor (Recommended):**\n\n[🔗 Install in Cursor](cursor://anysphere.cursor-deeplink/mcp/install?name=cross-llm-mcp\u0026config=eyJjcm9zcy1sbG0tbWNwIjp7ImNvbW1hbmQiOiJucHgiLCJhcmdzIjpbIi15IiwiY3Jvc3MtbGxtLW1jcCJdfX0=)\n\n**Or install manually:**\n\n```bash\nnpm install -g cross-llm-mcp\n# Or from source:\ngit clone https://github.com/JamesANZ/cross-llm-mcp.git\ncd cross-llm-mcp \u0026\u0026 npm install \u0026\u0026 npm run build\n```\n\n## Features\n\n### 🤖 Individual LLM Tools\n\n- **`call-chatgpt`** – OpenAI\u0027s ChatGPT API\n- **`call-claude`** – Anthropic\u0027s Claude API\n- **`call-deepseek`** – DeepSeek API\n- **`call-gemini`** – Google\u0027s Gemini API\n- **`call-grok`** – xAI\u0027s Grok API\n- **`call-kimi`** – Moonshot AI\u0027s Kimi API\n- **`call-perplexity`** – Perplexity AI API\n- **`call-mistral`** – Mistral AI API\n\n### 🔄 Combined Tools\n\n- **`call-all-llms`** – Call all LLMs with the same prompt\n- **`call-llm`** – Call a specific provider by name\n\n### ⚙️ Preferences \u0026 Model Selection\n\n- **`get-user-preferences`** – Get current preferences\n- **`set-user-preferences`** – Set default model, cost preference, and tag-based preferences\n- **`get-models-by-tag`** – Find models by tag (coding, business, reasoning, math, creative, general)\n\n### 📝 Prompt Logging\n\n- **`get-prompt-history`** – View prompt history with filters\n- **`get-prompt-stats`** – Get statistics about prompt logs\n- **`delete-prompt-entries`** – Delete log entries by criteria\n- **`clear-prompt-history`** – Clear all prompt logs\n\n## Installation\n\n### Cursor (One-Click)\n\nClick the install link above or use:\n\n```\ncursor://anysphere.cursor-deeplink/mcp/install?name=cross-llm-mcp\u0026config=eyJjcm9zcy1sbG0tbWNwIjp7ImNvbW1hbmQiOiJucHgiLCJhcmdzIjpbIi15IiwiY3Jvc3MtbGxtLW1jcCJdfX0=\n```\n\nAfter installation, add your API keys in Cursor settings (see Configuration below).\n\n### Manual Installation\n\n**Requirements:** Node.js 18+ and npm",
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  false,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  true,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "harrison/ai-counsel",
        "url":  "https://github.com/harrison/ai-counsel",
        "kind":  "github",
        "list_desc":  "🐍 🏠 🍎 🪟 🐧 - Deliberative consensus engine enabling multi-round debate between AI models with structured voting, convergence detection, and persistent decision graph memory.",
        "repo":  "harrison/ai-counsel",
        "branch":  null,
        "subdir":  null,
        "readme_found":  false,
        "readme_url":  null,
        "signals":  {
                        "mentions_local":  false,
                        "mentions_cloud":  false,
                        "mentions_api_key":  false,
                        "mentions_obsidian":  false,
                        "mentions_vector":  false,
                        "mentions_graph":  false,
                        "mentions_web":  false,
                        "storage_markdown":  false,
                        "storage_sqlite":  false,
                        "storage_db":  false
                    }
    },
    {
        "name":  "agentic-mcp-tools/memora",
        "url":  "https://github.com/agentic-mcp-tools/memora",
        "kind":  "github",
        "list_desc":  "Persistent memory with knowledge graph visualization, semantic/hybrid search, cloud sync (S3/R2), and cross-session context management.",
        "repo":  "agentic-mcp-tools/memora",
        "branch":  null,
        "subdir":  null,
        "readme_found":  true,
        "readme_url":  "https://raw.githubusercontent.com/agentic-mcp-tools/memora/main/README.md",
        "snippet":  "\u003cimg src=\"media/memora.gif\" width=\"60\" align=\"left\" alt=\"Memora Demo\"\u003e\n\n# Memora\n\n[![Version](https://img.shields.io/github/v/tag/agentic-mcp-tools/memora?label=version\u0026color=blue)](https://github.com/agentic-mcp-tools/memora/releases)\n[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)\n[![GitHub stars](https://img.shields.io/github/stars/agentic-mcp-tools/memora?style=social)](https://github.com/agentic-mcp-tools/memora/stargazers)\n\n\u003cbr clear=\"left\"\u003e\n\nA lightweight Model Context Protocol (MCP) server that persists shared memories in SQLite. Compatible with Claude Code, Codex CLI, and other MCP-aware clients.\n\n\u003cimg src=\"media/demo.gif\" alt=\"Memora Demo\" width=\"800\"\u003e\n\n## Features\n\n- **Persistent Storage** - SQLite-backed database with optional cloud sync (S3, GCS, Azure)\n- **Semantic Search** - Vector embeddings (TF-IDF, sentence-transformers, or OpenAI)\n- **Event Notifications** - Poll-based system for inter-agent communication\n- **Advanced Queries** - Full-text search, date ranges, tag filters (AND/OR/NOT)\n- **Cross-references** - Auto-linked related memories based on similarity\n- **Hierarchical Organization** - Explore memories by section/subsection\n- **Export/Import** - Backup and restore with merge strategies\n- **Knowledge Graph** - Interactive HTML visualization with filtering\n- **Live Graph Server** - Auto-starts HTTP server for remote access via SSH\n- **Statistics \u0026 Analytics** - Tag usage, trends, and connection insights\n- **Zero Dependencies** - Works out-of-box with Python stdlib (optional backends available)\n\n## Install\n\n```bash\n# From GitHub\npip install git+https://github.com/agentic-mcp-tools/memora.git\n\n# With extras\npip install -e \".[cloud]\"       # S3/R2/GCS cloud storage (boto3)\npip install -e \".[embeddings]\"  # semantic search (sentence-transformers)\npip install -e \".[all]\"         # cloud + embeddings + dev tools\n```\n\n## Usage\n\nThe server runs automatically when configured in Claude Code. Manual invocation:\n\n```bash\n# Default (stdio mode for MCP)\nmemora-server\n\n# With graph visualization server\nmemora-server --graph-port 8765\n\n# HTTP transport (alternative to stdio)\nmemora-server --transport streamable-http --host 127.0.0.1 --port 8080\n```\n\n## Claude Code Config\n\nAdd to `.mcp.json` in your project root:\n\n### Local DB\n```json\n{\n  \"mcpServers\": {\n    \"memora\": {\n      \"command\": \"memora-server\",\n      \"args\": [],\n      \"env\": {\n        \"MEMORA_DB_PATH\": \"~/.local/share/memora/memories.db\",\n        \"MEMORA_ALLOW_ANY_TAG\": \"1\",\n        \"MEMORA_GRAPH_PORT\": \"8765\"\n      }\n    }\n  }\n}\n```\n\n### Cloud DB (S3/R2)\n```json\n{\n  \"mcpServers\": {",
        "signals":  {
                        "mentions_local":  true,
                        "mentions_cloud":  true,
                        "mentions_api_key":  true,
                        "mentions_obsidian":  false,
                        "mentions_vector":  true,
                        "mentions_graph":  true,
                        "mentions_web":  true,
                        "storage_markdown":  false,
                        "storage_sqlite":  true,
                        "storage_db":  false
                    }
    }
]
